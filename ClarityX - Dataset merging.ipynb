{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cc0135",
   "metadata": {},
   "source": [
    "# ClarityX — Report\n",
    "\n",
    "**Title:** Dataset Merging, Location Resolution, Nearest-Region Mapping, and Asset Valuation  \n",
    "\n",
    "**Notebook:** `ClarityX - Dataset merging.ipynb`   \n",
    "\n",
    "---\n",
    "\n",
    "# Executive summary\n",
    "\n",
    "This report documents the data preparation, merging, geospatial matching, and valuation pipeline implemented in the notebook. The objective was to consolidate multiple RWAP datasets, resolve location-level conflicts (ZIP / city / state), derive region-level price metadata, map assets to nearest regions using geospatial nearest-neighbor search, and produce an imputed asset valuation table. The pipeline produces audit outputs that make the resolution decisions traceable and a final per-asset valuation suitable for downstream analysis.\n",
    "\n",
    "**Key final outputs (files written by the notebook):**\n",
    "- `final_asset_valuation.csv` — per-asset estimated valuation  \n",
    "- `assets_nearest_regions_raw.csv` — nearest-region matches with rank and distance  \n",
    "- `dataset1_merged_step.csv` — intermediate merged dataset  \n",
    "- `city_resolution_audit.csv`, `state_resolution_audit.csv`, `conflict_resolved.csv` — audit and conflict resolution records  \n",
    "\n",
    "---\n",
    "\n",
    "# Data sources\n",
    "\n",
    "Explicitly referenced input files (loaded via `pd.read_*` in the notebook):\n",
    "- `rwap25_gis_dataset1.csv`  \n",
    "- `rwap25_gis_dataset2.csv`  \n",
    "- `dataset1_merged_step.csv`  \n",
    "- `conflict_resolved.csv`  \n",
    "- `state_resolution_audit.csv`  \n",
    "\n",
    "These files contain asset records, geographic coordinates, previously merged artifacts, and conflict-resolution logs used as inputs to the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# Methodology — \n",
    "\n",
    "1. **Environment and imports**  \n",
    "   - Standard data science and geospatial libraries were imported (`pandas`, `numpy`, scikit-learn neighbors (`BallTree` / `NearestNeighbors`), `pgeocode`, `fuzzywuzzy`, `folium`, math utilities for haversine calculations, and `pathlib`).  \n",
    "\n",
    "2. **Canonicalization of location fields**  \n",
    "   - ZIP fields were standardized (removal of extraneous characters, normalization of numeric representations, preservation of leading zeros).  \n",
    "   - City and state fields were normalized for text comparisons (case standardization, punctuation removal) and prepared for conflict detection.  \n",
    "\n",
    "3. **Conflict detection and resolution**  \n",
    "   - Differing city/state values across sources were flagged.  \n",
    "   - Conflict decisions produced `Unified_City` / `Final_City` fields, with provenance recorded in columns such as `city_source`, `state_source`.  \n",
    "   - Audit outputs (`city_resolution_audit.csv`, etc.) captured these decisions for traceability.  \n",
    "\n",
    "4. **Region-level aggregation**  \n",
    "   - Price statistics and centroids (`region_lat`, `region_lon`) were aggregated at the ZIP/region level.  \n",
    "   - Features included `latest_price` and short-window averages.  \n",
    "\n",
    "5. **Dataset merging**  \n",
    "   - Enrichments were performed through left joins that attach region metadata to assets.  \n",
    "   - Some joins used `validate='m:1'` to prevent unintended many-to-many expansions.  \n",
    "\n",
    "6. **Geospatial nearest-neighbor matching**  \n",
    "   - A BallTree indexed region centroids (converted to radians) using the haversine metric.  \n",
    "   - Assets with valid coordinates were queried for K nearest regions.  \n",
    "   - Results were stored in `assets_nearest_regions_raw.csv` with neighbor ranks and distances.  \n",
    "\n",
    "7. **Valuation derivation**  \n",
    "   - Asset valuations (`estimated_price`) were derived using region-level price statistics from nearest regions.  \n",
    "   - Results were saved in `final_asset_valuation.csv`.  \n",
    "\n",
    "---\n",
    "\n",
    "# Features produced and their roles\n",
    "\n",
    "- `zip_std` / `Zip_raw`: standardized ZIP for joins  \n",
    "- `Unified_City`, `Final_City`, `city_source`: consolidated city values and provenance  \n",
    "- `State_unified_raw`, `state_source`: unified state values and provenance  \n",
    "- `region_zip`, `RegionName_raw`: identifiers for region grouping  \n",
    "- `region_lat`, `region_lon`: region centroid coordinates  \n",
    "- `asset_lat`, `asset_lon`: asset coordinates  \n",
    "- `neighbor_rank`, `distance`: rank and great-circle distance from asset to region centroid  \n",
    "- `latest_price`, `latest_navg`: region-level price summaries  \n",
    "- `estimated_price`: final imputed value for each asset  \n",
    "\n",
    "---\n",
    "\n",
    "# Observations\n",
    "\n",
    "- **Traceability:** Audit CSVs record conflict resolution decisions, supporting reproducibility.  \n",
    "- **Geospatial approach:** Relies on nearest-neighbor matching and local price statistics rather than supervised ML, ensuring interpretability.  \n",
    "- **Cardinality safeguards:** Use of `validate='m:1'` highlights careful handling of joins.  \n",
    "- **Distance units:** BallTree haversine output requires multiplication by Earth radius to interpret distances in kilometers.  \n",
    "- **Missing coordinates:** Assets without latitude/longitude were excluded from matching. Handling missingness is necessary for complete coverage.  \n",
    "- **Source harmonization:** Deterministic rules selected between conflicting values, with audit logs to support review.  \n",
    "\n",
    "---\n",
    "\n",
    "# Findings\n",
    "\n",
    "- Multi-source location data was successfully consolidated and unified.  \n",
    "- Per-asset valuations were derived transparently via geospatial nearest-neighbor propagation of regional price signals.  \n",
    "- Audit outputs provide a detailed trail for quality assurance and conflict verification.  \n",
    "- The pipeline outputs both detailed neighbor-match tables and clean final valuations for analysis.  \n",
    "\n",
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The notebook implements a robust, auditable pipeline for merging heterogeneous location datasets, resolving inconsistencies, and producing asset valuations via geospatial nearest-neighbor matching to region price statistics.  \n",
    "\n",
    "Outputs are well suited for both diagnostic review (audit and neighbor tables) and operational use (per-asset valuation file). The pipeline emphasizes interpretability, reproducibility, and transparency, making each valuation traceable to its geographic and statistical origins.  \n",
    "\n",
    "---\n",
    "\n",
    "*End of report.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1b288-eca3-4dc7-b2ec-0ad5f8c2342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64c448-608b-4d5b-b41d-4af19df5c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073285c-5e91-460e-bed8-8276af501043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — load the two CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bd69b-62ef-471a-9582-512b6c023574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF1 rows,cols: (8652, 18)\n",
      "DF2 rows,cols: (26314, 316)\n",
      "DF1 columns: ['Location Code', 'Real Property Asset Name', 'Installation Name', 'Owned or Leased', 'GSA Region', 'Street Address', 'City', 'State', 'Zip Code', 'Latitude', 'Longitude', 'Building Rentable Square Feet', 'Available Square Feet', 'Construction Date', 'Congressional District', 'Congressional District Representative Name', 'Building Status', 'Real Property Asset Type']\n",
      "DF2 columns (sample): ['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', 'State', 'City', 'Metro', 'CountyName', '31-01-2000', '29-02-2000', '31-03-2000']\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('rwap25_gis_dataset1.csv')   # assets\n",
    "df2 = pd.read_csv('rwap25_gis_dataset2.csv')   # zip time-series\n",
    "\n",
    "print(\"DF1 rows,cols:\", df1.shape)\n",
    "print(\"DF2 rows,cols:\", df2.shape)\n",
    "print(\"DF1 columns:\", df1.columns.tolist())\n",
    "print(\"DF2 columns (sample):\", df2.columns[:12].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f2833-8415-464e-9ab3-c764862a4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — keep only zip-level rows from Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69f4c0-a2d6-4163-9807-ad1db61bc1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered df2 to zip rows: (26314, 316)\n"
     ]
    }
   ],
   "source": [
    "# If RegionType exists, filter rows where it's 'zip' (case-insensitive).\n",
    "if 'RegionType' in df2.columns:\n",
    "    df2_zip = df2[df2['RegionType'].astype(str).str.lower() == 'zip'].copy()\n",
    "else:\n",
    "    df2_zip = df2.copy()\n",
    "\n",
    "print(\"Filtered df2 to zip rows:\", df2_zip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360af3e-a50b-43f1-8a77-a7cbca939742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — standardize ZIP strings in both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd116ba5-e116-4c78-bf4c-7ef95664746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique zips in df1: 3581\n",
      "Unique zips in df2_zip: 26314\n"
     ]
    }
   ],
   "source": [
    "# For df1: create a standardized 'zip' column from 'Zip Code'\n",
    "if 'Zip Code' in df1.columns:\n",
    "    df1['Zip_raw'] = df1['Zip Code'].astype(str).str.strip().fillna('')\n",
    "else:\n",
    "    # fallback: take first column name that contains 'zip'\n",
    "    zip_col = [c for c in df1.columns if 'zip' in c.lower()]\n",
    "    df1['Zip_raw'] = df1[zip_col[0]].astype(str).str.strip() if zip_col else ''\n",
    "\n",
    "df1['zip'] = df1['Zip_raw'].str.extract(r'(\\d+)', expand=False).fillna('').str.zfill(5)\n",
    "\n",
    "# For df2_zip: extract numeric from RegionName -> zip\n",
    "if 'RegionName' in df2_zip.columns:\n",
    "    df2_zip['RegionName_raw'] = df2_zip['RegionName'].astype(str).str.strip().fillna('')\n",
    "    df2_zip['zip'] = df2_zip['RegionName_raw'].str.extract(r'(\\d+)', expand=False).fillna('').str.zfill(5)\n",
    "else:\n",
    "    # fallback: find any column that looks like RegionName\n",
    "    cand = [c for c in df2_zip.columns if 'region' in c.lower() or 'zip' in c.lower()]\n",
    "    if cand:\n",
    "        df2_zip['RegionName_raw'] = df2_zip[cand[0]].astype(str).str.strip().fillna('')\n",
    "        df2_zip['zip'] = df2_zip['RegionName_raw'].str.extract(r'(\\d+)', expand=False).fillna('').str.zfill(5)\n",
    "    else:\n",
    "        df2_zip['RegionName_raw'] = ''\n",
    "        df2_zip['zip'] = ''\n",
    "\n",
    "print(\"Unique zips in df1:\", df1['zip'].nunique())\n",
    "print(\"Unique zips in df2_zip:\", df2_zip['zip'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8457c9-f3ae-42ac-a0d8-9c7ab3c54e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 — find the date columns in Dataset 2 and melt it to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f2706-d425-4858-b7c8-e8c1b5c9750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found date columns count: 307\n",
      "Sample date columns: ['31-01-2000', '29-02-2000', '31-03-2000', '30-04-2000', '31-05-2000', '30-06-2000']\n",
      "df2_long rows: (8078398, 15)\n"
     ]
    }
   ],
   "source": [
    "# find columns matching dd-mm-yyyy (e.g. '31-07-2025')\n",
    "date_pattern = re.compile(r'^\\d{2}-\\d{2}-\\d{4}$')\n",
    "date_cols = [c for c in df2_zip.columns if date_pattern.match(c)]\n",
    "\n",
    "print(\"Found date columns count:\", len(date_cols))\n",
    "print(\"Sample date columns:\", date_cols[:6])\n",
    "\n",
    "# melt wide->long: id_vars are everything except date columns\n",
    "id_vars = [c for c in df2_zip.columns if c not in date_cols]\n",
    "df2_long = df2_zip.melt(id_vars=id_vars, value_vars=date_cols,\n",
    "                        var_name='date_str', value_name='price_raw')\n",
    "\n",
    "# parse date and numeric price\n",
    "df2_long['date'] = pd.to_datetime(df2_long['date_str'], format='%d-%m-%Y', errors='coerce')\n",
    "df2_long['price'] = pd.to_numeric(df2_long['price_raw'], errors='coerce')\n",
    "\n",
    "# drop invalid date rows (if any)\n",
    "df2_long = df2_long.dropna(subset=['date']).copy()\n",
    "\n",
    "print(\"df2_long rows:\", df2_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8921e56-aea9-4e04-ad28-4b2242e8dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 — compute the latest non-null price and its date per ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c9a58-5f54-41b6-8c9c-898c0d5bc03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zips with latest price available: 26314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>price_latest</th>\n",
       "      <th>price_latest_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>340459.1165</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01002</td>\n",
       "      <td>538321.8056</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01005</td>\n",
       "      <td>409267.9918</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01007</td>\n",
       "      <td>467503.3545</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01008</td>\n",
       "      <td>372725.8783</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     zip  price_latest price_latest_date\n",
       "0  01001   340459.1165        2025-07-31\n",
       "1  01002   538321.8056        2025-07-31\n",
       "2  01005   409267.9918        2025-07-31\n",
       "3  01007   467503.3545        2025-07-31\n",
       "4  01008   372725.8783        2025-07-31"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only rows where price is not null, then take the last (latest date) per zip\n",
    "df2_long_nonnull = df2_long.dropna(subset=['price']).copy()\n",
    "\n",
    "# if there are no non-null prices we'll get an empty df\n",
    "if df2_long_nonnull.empty:\n",
    "    print(\"Warning: dataset2 has no non-null price values.\")\n",
    "    latest_per_zip = pd.DataFrame(columns=['zip','price_latest','price_latest_date'])\n",
    "else:\n",
    "    latest_per_zip = (df2_long_nonnull.sort_values(['zip','date'])\n",
    "                                  .groupby('zip', as_index=False)\n",
    "                                  .last()[['zip','price','date']])\n",
    "    latest_per_zip = latest_per_zip.rename(columns={'price':'price_latest','date':'price_latest_date'})\n",
    "\n",
    "print(\"Zips with latest price available:\", len(latest_per_zip))\n",
    "latest_per_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf4e0d-6c28-4dfa-9995-51a2a6e4c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 — merge latest-per-zip info back with meta (City/State) and attach to Dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610328b2-4568-4f32-b81a-9cc1bcf4faed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged rows: (8652, 27)\n",
      "Fraction with price_latest: 0.9288025889967637\n"
     ]
    }
   ],
   "source": [
    "# pick some metadata from df2_zip (if present)\n",
    "meta_cols = []\n",
    "for c in ['RegionID','City','State','CountyName']:\n",
    "    if c in df2_zip.columns:\n",
    "        meta_cols.append(c)\n",
    "meta_cols = list(dict.fromkeys(meta_cols))  # keep order & unique\n",
    "\n",
    "zip_meta = df2_zip[meta_cols + ['zip']].drop_duplicates(subset=['zip'], keep='first')\n",
    "\n",
    "# create zip_info: meta + latest price\n",
    "zip_info = zip_meta.merge(latest_per_zip, on='zip', how='left')\n",
    "\n",
    "# left-merge into df1 (many assets per zip -> one zip_info row)\n",
    "merged = df1.merge(zip_info, on='zip', how='left', validate='m:1')\n",
    "\n",
    "# initial source flag: exact_zip if we have a price_latest\n",
    "merged['source'] = np.where(merged['price_latest'].notna(), 'exact_zip', np.nan)\n",
    "\n",
    "print(\"Merged rows:\", merged.shape)\n",
    "print(\"Fraction with price_latest:\", merged['price_latest'].notna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad7c9c-e17d-415e-9934-ca95f14298dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 — inspect assets that don’t have price_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7305a092-220f-4c22-98fe-8cb80b7c1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of assets missing price_latest: 616\n",
      "Sample missing zips (up to 50): ['20993', '47907', '85620', '80225', '00968', '70803', '59482', '78567', '20192', '59256', '88029', '83853', '24011', '96799', '99752', '59542', '92283', '05460', '76155', '08608', '97204', '59411', '04491', '10278', '52801', '20373', '99780', '58329', '00716', '00708', '20585', '00820', '04936', '00641', '20503', '00784', '85633', '10038', '79711', '14604', '77010', '63145', '87026', '20201', '79839', '47405', '96950', '00918', '78235', '00824']\n",
      "Empty DataFrame\n",
      "Columns: [zip, n_obs, n_non_null, first_date, last_date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "missing = merged[merged['price_latest'].isna()].copy()\n",
    "print(\"Number of assets missing price_latest:\", len(missing))\n",
    "# show unique missing zips and their df2_long summary\n",
    "missing_zips = missing['zip'].unique().tolist()[:50]\n",
    "print(\"Sample missing zips (up to 50):\", missing_zips)\n",
    "\n",
    "# For diagnostics: show df2_long info for those zips\n",
    "diag = (df2_long[df2_long['zip'].isin(missing_zips)]\n",
    "        .groupby('zip').agg(n_obs=('price','size'),\n",
    "                            n_non_null=('price', lambda s: s.notna().sum()),\n",
    "                            first_date=('date','min'),\n",
    "                            last_date=('date','max')).reset_index())\n",
    "print(diag.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08acdc4-c0d7-4339-a3e4-83cf772a8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 — save the merged file (with source) so you have a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d9b8f-66b8-4193-b472-63296b8be3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged file to dataset1_merged_step.csv\n"
     ]
    }
   ],
   "source": [
    "merged.to_csv('dataset1_merged_step.csv', index=False)\n",
    "print(\"Saved merged file to dataset1_merged_step.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b85dc4-6ae5-439a-84f5-690f2ea1e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP A — quick read/setup (if not done already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d366b-3c57-453d-b596-437563632c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "merged = pd.read_csv('dataset1_merged_step.csv')  # file from earlier step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bc4e7-1900-4f7e-8004-e14ac32a4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a36e5-c9e6-4efa-945d-fefa549c4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspect (see what's filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a80f5-004d-4359-9963-84d2040eefb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-null counts:\n",
      "State_x: 8652 / 8652\n",
      "State_y: 8036 / 8652\n",
      "\n",
      "Sample pairs (first 20 rows):\n",
      "   State_x State_y\n",
      "0       GA      GA\n",
      "1       WI      WI\n",
      "2       MN      MN\n",
      "3       MD      MD\n",
      "4       CO      CO\n",
      "5       CO      CO\n",
      "6       FL      FL\n",
      "7       AZ      AZ\n",
      "8       FL      FL\n",
      "9       MD     NaN\n",
      "10      CA      CA\n",
      "11      CA      CA\n",
      "12      IN     NaN\n",
      "13      AZ      AZ\n",
      "14      PA      PA\n",
      "15      UT      UT\n",
      "16      FL      FL\n",
      "17      MD      MD\n",
      "18      PA      PA\n",
      "19      AZ      AZ\n"
     ]
    }
   ],
   "source": [
    "# show counts and a sample\n",
    "print(\"Non-null counts:\")\n",
    "print(\"State_x:\", merged['State_x'].notna().sum(), \"/\", len(merged))\n",
    "print(\"State_y:\", merged['State_y'].notna().sum(), \"/\", len(merged))\n",
    "\n",
    "print(\"\\nSample pairs (first 20 rows):\")\n",
    "print(merged[['State_x','State_y']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440111b-290e-4aa6-b646-1af4ff69c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1 — Create unified State and state_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c485568-0bfd-439c-b8ed-0c196e7310d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified State non-null: 8652 / 8652\n",
      "state_source counts:\n",
      " state_source\n",
      "both_same        8035\n",
      "state_x           616\n",
      "both_conflict       1\n",
      "Name: count, dtype: int64\n",
      "Sample unified states: ['GA' 'WI' 'MN' 'MD' 'CO' 'FL' 'AZ' 'CA' 'IN' 'PA' 'UT' 'TX' 'MO' 'NM'\n",
      " 'OR' 'VA' 'WY' 'LA' 'KY' 'MI']\n"
     ]
    }
   ],
   "source": [
    "# run this first (assumes merged DataFrame exists)\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "# prefer State_x, else State_y\n",
    "merged['State_unified_raw'] = merged['State_x'].where(\n",
    "    merged['State_x'].notna() & (merged['State_x'].astype(str).str.strip()!=''),\n",
    "    merged['State_y']\n",
    ")\n",
    "\n",
    "# normalize whitespace and uppercase, convert empty/'nan' strings to None\n",
    "merged['State_unified_raw'] = merged['State_unified_raw'].astype(str).str.strip().replace({'': None, 'nan': None, 'None': None})\n",
    "merged['State_unified_raw'] = merged['State_unified_raw'].where(merged['State_unified_raw'].notna(), None)\n",
    "merged['State_unified_raw'] = merged['State_unified_raw'].apply(lambda x: x.upper() if pd.notna(x) else x)\n",
    "\n",
    "# simple full-name -> abbrev mapping (extend if needed)\n",
    "state_map = {\n",
    "    'ALABAMA':'AL','ALASKA':'AK','ARIZONA':'AZ','ARKANSAS':'AR','CALIFORNIA':'CA',\n",
    "    'COLORADO':'CO','CONNECTICUT':'CT','DELAWARE':'DE','FLORIDA':'FL','GEORGIA':'GA',\n",
    "    'HAWAII':'HI','IDAHO':'ID','ILLINOIS':'IL','INDIANA':'IN','IOWA':'IA','KANSAS':'KS',\n",
    "    'KENTUCKY':'KY','LOUISIANA':'LA','MAINE':'ME','MARYLAND':'MD','MASSACHUSETTS':'MA',\n",
    "    'MICHIGAN':'MI','MINNESOTA':'MN','MISSISSIPPI':'MS','MISSOURI':'MO','MONTANA':'MT',\n",
    "    'NEBRASKA':'NE','NEVADA':'NV','NEW HAMPSHIRE':'NH','NEW JERSEY':'NJ','NEW MEXICO':'NM',\n",
    "    'NEW YORK':'NY','NORTH CAROLINA':'NC','NORTH DAKOTA':'ND','OHIO':'OH','OKLAHOMA':'OK',\n",
    "    'OREGON':'OR','PENNSYLVANIA':'PA','RHODE ISLAND':'RI','SOUTH CAROLINA':'SC',\n",
    "    'SOUTH DAKOTA':'SD','TENNESSEE':'TN','TEXAS':'TX','UTAH':'UT','VERMONT':'VT',\n",
    "    'VIRGINIA':'VA','WASHINGTON':'WA','WEST VIRGINIA':'WV','WISCONSIN':'WI','WYOMING':'WY',\n",
    "    'DISTRICT OF COLUMBIA':'DC', 'DC':'DC'\n",
    "}\n",
    "\n",
    "def to_state_abbrev(s):\n",
    "    if s is None: return None\n",
    "    s_up = str(s).strip().upper()\n",
    "    if len(s_up) == 2 and s_up.isalpha(): \n",
    "        return s_up\n",
    "    return state_map.get(s_up, s_up)  # fallback keep as-is for manual inspection\n",
    "\n",
    "merged['State'] = merged['State_unified_raw'].apply(to_state_abbrev)\n",
    "\n",
    "# mark source\n",
    "def state_source(row):\n",
    "    sx = row.get('State_x')\n",
    "    sy = row.get('State_y')\n",
    "    if pd.notna(sx) and str(sx).strip()!='' and pd.notna(sy) and str(sy).strip()!='':\n",
    "        if str(sx).strip().upper() == str(sy).strip().upper():\n",
    "            return 'both_same'\n",
    "        else:\n",
    "            return 'both_conflict'\n",
    "    if pd.notna(sx) and str(sx).strip()!='':\n",
    "        return 'state_x'\n",
    "    if pd.notna(sy) and str(sy).strip()!='':\n",
    "        return 'state_y'\n",
    "    return 'none'\n",
    "\n",
    "merged['state_source'] = merged.apply(state_source, axis=1)\n",
    "\n",
    "# Quick check\n",
    "print(\"Unified State non-null:\", merged['State'].notna().sum(), \"/\", len(merged))\n",
    "print(\"state_source counts:\\n\", merged['state_source'].value_counts())\n",
    "print(\"Sample unified states:\", merged['State'].unique()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6462b535-3855-48a5-a73b-12e752396a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the one conflict and resolve it (quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc2e8f-230f-4251-846b-dddc0fe2b262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conflicts: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location Code</th>\n",
       "      <th>State_x</th>\n",
       "      <th>State_y</th>\n",
       "      <th>City_x</th>\n",
       "      <th>City_y</th>\n",
       "      <th>zip</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>NJ0148</td>\n",
       "      <td>NJ</td>\n",
       "      <td>NY</td>\n",
       "      <td>NEWARK</td>\n",
       "      <td>New Windsor</td>\n",
       "      <td>12553</td>\n",
       "      <td>40.728366</td>\n",
       "      <td>-74.174679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Location Code State_x State_y  City_x       City_y    zip   Latitude  \\\n",
       "1497        NJ0148      NJ      NY  NEWARK  New Windsor  12553  40.728366   \n",
       "\n",
       "      Longitude  \n",
       "1497 -74.174679  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show conflicting rows where both exist but are different\n",
    "conflicts = merged[(merged['State_x'].notna()) & (merged['State_y'].notna()) &\n",
    "                   (merged['State_x'].astype(str).str.strip().str.upper() != merged['State_y'].astype(str).str.strip().str.upper())]\n",
    "\n",
    "print(\"Number of conflicts:\", len(conflicts))\n",
    "conflicts[['Location Code','State_x','State_y','City_x','City_y','zip','Latitude','Longitude']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af413276-1ac5-46b3-9ce4-be552c50db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solving the conflict - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff813885-b96b-49c5-a4ab-1dbddbe9b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install & import (only if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59a4a4-4288-4f98-babf-7a9c79a30254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgeocode\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83452a6c-4090-4047-84aa-3eaff4688fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ZIP-to-state lookup to get authoritative state code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86802a-30c3-42f9-a940-9297a4b980fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP 12553 -> NY\n"
     ]
    }
   ],
   "source": [
    "# create pgeocode nominatim object for US\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "# function to lookup 2-letter state by zip (returns None if not found)\n",
    "def zip_to_state_abbrev(zipcode):\n",
    "    try:\n",
    "        info = nomi.query_postal_code(str(zipcode).zfill(5))\n",
    "        # pgeocode returns NaN for missing fields; state_code property is abbreviation if present\n",
    "        st = info['state_code']\n",
    "        if pd.isna(st):\n",
    "            return None\n",
    "        return str(st).strip().upper()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# test on the conflict zip (12553)\n",
    "print(\"ZIP 12553 ->\", zip_to_state_abbrev('12553'))  # expected 'NY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde0efc-0472-4e4b-bfc9-12c404e560c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve all conflicts using the ZIP lookup and record provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406cbff6-a65e-4086-9ce7-ac4640b6ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply zip lookup for all conflicted rows and update the merged DataFrame\n",
    "for idx, row in conflicts.iterrows():\n",
    "    z = row['zip']\n",
    "    authoritative_state = zip_to_state_abbrev(z)\n",
    "    if authoritative_state:\n",
    "        # keep original values for audit\n",
    "        merged.at[idx, 'State_before_resolution'] = merged.at[idx, 'State']  # current unified state\n",
    "        merged.at[idx, 'State_resolved_by_zip'] = authoritative_state\n",
    "        # update unified State to authoritative zip result\n",
    "        merged.at[idx, 'State'] = authoritative_state\n",
    "        merged.at[idx, 'state_source'] = 'zip_lookup_override'\n",
    "    else:\n",
    "        # if the lookup failed, leave current State as-is but flag for manual review\n",
    "        merged.at[idx, 'State_resolve_note'] = 'zip_lookup_failed_manual_review'\n",
    "        merged.at[idx, 'state_source'] = 'conflict_needs_manual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831474e-0c97-4bcd-a0f8-2f56c3f7c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audit the change (show rows that were modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9344ba4-5b53-4666-88c1-a4a535b9d152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved rows via ZIP lookup: 1\n",
      "     Location Code    zip State_x State_y State_before_resolution  \\\n",
      "1497        NJ0148  12553      NJ      NY                      NJ   \n",
      "\n",
      "     State_resolved_by_zip State         state_source  \n",
      "1497                    NY    NY  zip_lookup_override  \n"
     ]
    }
   ],
   "source": [
    "# show those rows you just modified\n",
    "resolved = merged[merged['state_source']=='zip_lookup_override']\n",
    "print(\"Resolved rows via ZIP lookup:\", len(resolved))\n",
    "print(resolved[['Location Code','zip','State_x','State_y','State_before_resolution','State_resolved_by_zip','State','state_source']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c25fd-2e7d-4071-ae30-95969711bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an audit log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f09446-49f5-42f0-b7f6-0395091a1b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state resolution audit to state_resolution_audit.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the conflict-resolution audit for traceability\n",
    "audit = merged.loc[conflicts.index, ['Location Code','zip','State_x','State_y','State_before_resolution','State_resolved_by_zip','State','state_source']]\n",
    "audit.to_csv('state_resolution_audit.csv', index=False)\n",
    "print(\"Saved state resolution audit to state_resolution_audit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c45953-aee8-4d4a-8a16-66be75ddeb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b4a8d-4e09-410c-9c9e-1ee1b6e5bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (8652, 27)\n",
      "Columns available: Index(['Location Code', 'Real Property Asset Name', 'Installation Name',\n",
      "       'Owned or Leased', 'GSA Region', 'Street Address', 'City_x', 'State_x',\n",
      "       'Zip Code', 'Latitude', 'Longitude', 'Building Rentable Square Feet',\n",
      "       'Available Square Feet', 'Construction Date', 'Congressional District',\n",
      "       'Congressional District Representative Name', 'Building Status',\n",
      "       'Real Property Asset Type', 'Zip_raw', 'zip', 'RegionID', 'City_y',\n",
      "       'State_y', 'CountyName', 'price_latest', 'price_latest_date', 'source'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged dataset (replace with your merged CSV file name)\n",
    "df = pd.read_csv(\"dataset1_merged_step.csv\")\n",
    "\n",
    "print(\"Initial shape:\", df.shape)\n",
    "print(\"Columns available:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd6d58-fc1a-4ce2-b8a9-c9ad92ff3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_state(row):\n",
    "    # If both match → use either\n",
    "    if pd.notna(row['State_x']) and pd.notna(row['State_y']):\n",
    "        if row['State_x'] == row['State_y']:\n",
    "            return row['State_x'], 'both_same'\n",
    "        else:\n",
    "            return row['State_x'], 'both_conflict'\n",
    "    # If only State_x exists\n",
    "    elif pd.notna(row['State_x']):\n",
    "        return row['State_x'], 'state_x'\n",
    "    # If only State_y exists\n",
    "    elif pd.notna(row['State_y']):\n",
    "        return row['State_y'], 'state_y'\n",
    "    # If both are null\n",
    "    else:\n",
    "        return None, 'missing'\n",
    "\n",
    "df[['Unified_State', 'state_source']] = df.apply(resolve_state, axis=1, result_type=\"expand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821991dc-bc22-43cf-9fc2-b0e89c18c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conflicts: 1\n",
      "     Location Code State_x State_y  City_x       City_y    zip\n",
      "1497        NJ0148      NJ      NY  NEWARK  New Windsor  12553\n"
     ]
    }
   ],
   "source": [
    "conflicts = df[df['state_source'] == 'both_conflict']\n",
    "print(f\"Number of conflicts: {len(conflicts)}\")\n",
    "\n",
    "if len(conflicts) > 0:\n",
    "    print(conflicts[['Location Code','State_x','State_y','City_x','City_y','zip']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d97eb-7581-4eb9-9856-26b3744483fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NJ0148, correct Unified_State to NJ\n",
    "df.loc[df['Location Code'] == 'NJ0148', 'Unified_State'] = 'NJ'\n",
    "df.loc[df['Location Code'] == 'NJ0148', 'state_source'] = 'resolved_conflict'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314452c-8a13-4e1a-a1a7-9eb0fac0f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['State_x', 'State_y'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e85c6-b259-40b8-b5af-6a6959559a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean dataset saved as: state_resolution_audit.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"state_resolution_audit.csv\", index=False)\n",
    "print(\"✅ Clean dataset saved as: state_resolution_audit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45103af8-5a55-47d0-9b49-71286a655c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unifying the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a6faa-8a72-4441-b836-da1406a80b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Load the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6884f-2a21-4ed6-bae7-a34296be6d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (8652, 27)\n",
      "Columns: Index(['Location Code', 'Real Property Asset Name', 'Installation Name',\n",
      "       'Owned or Leased', 'GSA Region', 'Street Address', 'City_x', 'Zip Code',\n",
      "       'Latitude', 'Longitude', 'Building Rentable Square Feet',\n",
      "       'Available Square Feet', 'Construction Date', 'Congressional District',\n",
      "       'Congressional District Representative Name', 'Building Status',\n",
      "       'Real Property Asset Type', 'Zip_raw', 'zip', 'RegionID', 'City_y',\n",
      "       'CountyName', 'price_latest', 'price_latest_date', 'source',\n",
      "       'Unified_State', 'state_source'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset from the previous step\n",
    "df = pd.read_csv(\"state_resolution_audit.csv\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f619b88-f645-4ecf-8330-5f7e7f72f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — Create Unified_City column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc3cb6-6c23-462f-a27e-9c4b192ba298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll define rules for merging City_x and City_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1d18f-cd74-4287-8972-ea3e60d837c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_city(row):\n",
    "    city_x = str(row['City_x']).strip() if pd.notna(row['City_x']) else None\n",
    "    city_y = str(row['City_y']).strip() if pd.notna(row['City_y']) else None\n",
    "\n",
    "    # If both are present and same → use either\n",
    "    if city_x and city_y:\n",
    "        if city_x.lower() == city_y.lower():  # case-insensitive match\n",
    "            return city_x, 'both_same'\n",
    "        else:\n",
    "            return city_x, 'both_conflict'  # temporarily mark conflict\n",
    "\n",
    "    # If only city_x exists\n",
    "    elif city_x:\n",
    "        return city_x, 'city_x'\n",
    "\n",
    "    # If only city_y exists\n",
    "    elif city_y:\n",
    "        return city_y, 'city_y'\n",
    "\n",
    "    # If both are missing\n",
    "    else:\n",
    "        return None, 'missing'\n",
    "\n",
    "# Apply function to dataframe\n",
    "df[['Unified_City', 'city_source']] = df.apply(resolve_city, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c42539-bef3-407c-bf28-58bd1753a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Check conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208c8a9-9e25-4234-a64d-e9828eaf64d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of city conflicts: 660\n",
      "    Location Code                 City_x              City_y    zip\n",
      "26         NM0576               PLACITAS        Santa Teresa  88008\n",
      "48         MI2137               MUSKEGON      Roosevelt Park  49441\n",
      "63         NY7402               BROOKLYN            New York  11201\n",
      "75         AR0062     HOT SPGS NATL PARK         Hot Springs  71901\n",
      "87         IL2520             BELLEVILLE              Shiloh  62221\n",
      "91         NC2314        WASHINGTON PARK          Washington  27889\n",
      "98         IL2491            SPRINGFIELD        Leland Grove  62704\n",
      "101        MO0617              ST. LOUIS         Saint Louis  63120\n",
      "128        MO0610              ST. LOUIS         Saint Louis  63120\n",
      "157        GA1158                ATLANTA            Chamblee  30341\n",
      "168        MI2187  CHESTERFIELD TOWNSHIP        Chesterfield  48051\n",
      "175        FL3183                  MIAMI       The Crossings  33186\n",
      "184        OH2418              BEACHWOOD      Shaker Heights  44122\n",
      "186        NJ5116              MT LAUREL  Mt Laurel Township   8054\n",
      "215        VT0756             NORTH TROY                Troy   5859\n",
      "225        MO1812              ST. LOUIS         Saint Louis  63101\n",
      "229        VA3056                HENRICO            Lakeside  23228\n",
      "234        NJ4525                MARLTON    Evesham Township   8053\n",
      "241        FL3206                  MIAMI       The Crossings  33186\n",
      "242        VT0757             NORTH TROY                Troy   5859\n"
     ]
    }
   ],
   "source": [
    "conflicts = df[df['city_source'] == 'both_conflict']\n",
    "print(f\"Number of city conflicts: {len(conflicts)}\")\n",
    "\n",
    "# Show a sample of mismatched cities\n",
    "print(conflicts[['Location Code', 'City_x', 'City_y', 'zip']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce46918-a1a9-4b2f-bc6d-a7154ad073a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of your city conflicts aren’t really “true” conflicts but spelling/style/abbreviation differences or neighborhood vs. city names.\n",
    "\n",
    "# We'll handle these smartly using the following stepwise approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c76f7f-d080-40b4-9b8b-afee95af714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Use ZIP Codes to Auto-Resolve Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64606523-aa3d-4f81-ac23-37721b326a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining conflicts after ZIP-based resolution: 688\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"state_resolution_audit.csv\")\n",
    "\n",
    "# If city_y is missing, first fill it with city_x\n",
    "df[\"City_y\"] = df[\"City_y\"].fillna(df[\"City_x\"])\n",
    "\n",
    "# Create a ZIP → most common city mapping\n",
    "zip_city_map = (\n",
    "    df.groupby(\"zip\")[\"City_y\"]\n",
    "    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Apply this mapping to create a unified city column\n",
    "df[\"Unified_City\"] = df[\"zip\"].map(zip_city_map)\n",
    "\n",
    "# Check how many are still conflicting\n",
    "df[\"city_conflict\"] = (df[\"City_x\"].str.upper() != df[\"Unified_City\"].str.upper())\n",
    "print(\"Remaining conflicts after ZIP-based resolution:\", df[\"city_conflict\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae0b56-8983-4fb2-87b1-f3b93939ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — Fuzzy Matching for Close Spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37573756-b5a7-4a97-b6fd-355d0c8d67af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def pick_best_city(row):\n",
    "    city_x = str(row[\"City_x\"]).upper()\n",
    "    unified = str(row[\"Unified_City\"]).upper()\n",
    "    score = fuzz.ratio(city_x, unified)\n",
    "    return unified if score >= 85 else city_x\n",
    "\n",
    "df[\"Final_City\"] = df.apply(pick_best_city, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22325914-c991-4553-a13f-283292497adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Save Final Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527623e-993c-4e14-8d9c-4f16bb8db98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ City conflicts resolved and saved to city_resolution_audit.csv\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=[\"city_conflict\"], inplace=True, errors=\"ignore\")\n",
    "df.to_csv(\"city_resolution_audit.csv\", index=False)\n",
    "print(\"✅ City conflicts resolved and saved to city_resolution_audit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0067a4e-a0df-46a1-98f2-05b4b7502e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 — Recheck Conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf1a6f-4a20-424b-8344-4e7c28d8d27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final remaining conflicts: 38\n",
      "     Location Code              City_x                City_y  \\\n",
      "348         NC2615       WINSTON SALEM         Winston-Salem   \n",
      "384         NC0113       WINSTON SALEM         Winston-Salem   \n",
      "462         OH2481        BEAVER CREEK           Beavercreek   \n",
      "1017        AR0066  HELENA-WEST HELENA  Helena - West Helena   \n",
      "1716        HI8562              LIHU'E                 Lihue   \n",
      "1886        MI3087    ST. CLAIR SHORES    Saint Clair Shores   \n",
      "2066        HI7553              LIHU'E                 Lihue   \n",
      "2160        MI3033     SAULT STE MARIE    Sault Sainte Marie   \n",
      "2216        SC1374         MT PLEASANT        Mount Pleasant   \n",
      "2244        NC1383       WINSTON SALEM         Winston-Salem   \n",
      "2304        HI7793              KEA'AU                 Keaau   \n",
      "2535        PA0627        WILKES BARRE          Wilkes-Barre   \n",
      "2588        PA0921        WILKES BARRE          Wilkes-Barre   \n",
      "2716        OH1816     ST. CLAIRSVILLE     Saint Clairsville   \n",
      "2995        PA0920        WILKES BARRE          Wilkes-Barre   \n",
      "3107        MN1711       SOUTH ST PAUL      South Saint Paul   \n",
      "3229        PA0477        WILKES BARRE          Wilkes-Barre   \n",
      "3543        PA0808             DU BOIS                DuBois   \n",
      "3596        PA0591        WILKES BARRE          Wilkes-Barre   \n",
      "4264        HI8328              LIHU'E                 Lihue   \n",
      "4308        MI2184     SAULT STE MARIE    Sault Sainte Marie   \n",
      "4544        MD0259        DISTRICT HTS      District Heights   \n",
      "4592        MN1726       SOUTH ST PAUL      South Saint Paul   \n",
      "4784        IL2628            O'FALLON              O Fallon   \n",
      "5167        VT8116       ST. JOHNSBURY          St Johnsbury   \n",
      "5254        HI5889              LIHU'E                 Lihue   \n",
      "6265        PA0676        WILKES BARRE          Wilkes-Barre   \n",
      "6791        MI0724     SAULT STE MARIE    Sault Sainte Marie   \n",
      "6939        NC2711       WINSTON SALEM         Winston-Salem   \n",
      "6991        NC2595       WINSTON SALEM         Winston-Salem   \n",
      "7040        MI2151     SAULT STE MARIE    Sault Sainte Marie   \n",
      "7762        SC2318         MT PLEASANT        Mount Pleasant   \n",
      "7853        PA0630        WILKES BARRE          Wilkes-Barre   \n",
      "7979        MI3019     SAULT STE MARIE    Sault Sainte Marie   \n",
      "8044        VT8118       ST. JOHNSBURY          St Johnsbury   \n",
      "8093        HI7501              LIHU'E                 Lihue   \n",
      "8194        MI2145     SAULT STE MARIE    Sault Sainte Marie   \n",
      "8249        SC2172     HILTON HEAD ISL    Hilton Head Island   \n",
      "\n",
      "                Final_City    zip  \n",
      "348          WINSTON-SALEM  27101  \n",
      "384          WINSTON-SALEM  27101  \n",
      "462            BEAVERCREEK  45431  \n",
      "1017  HELENA - WEST HELENA  72342  \n",
      "1716                 LIHUE  96766  \n",
      "1886    SAINT CLAIR SHORES  48080  \n",
      "2066                 LIHUE  96766  \n",
      "2160    SAULT SAINTE MARIE  49783  \n",
      "2216        MOUNT PLEASANT  29464  \n",
      "2244         WINSTON-SALEM  27101  \n",
      "2304                 KEAAU  96749  \n",
      "2535          WILKES-BARRE  18702  \n",
      "2588          WILKES-BARRE  18702  \n",
      "2716     SAINT CLAIRSVILLE  43950  \n",
      "2995          WILKES-BARRE  18702  \n",
      "3107      SOUTH SAINT PAUL  55075  \n",
      "3229          WILKES-BARRE  18702  \n",
      "3543                DUBOIS  15801  \n",
      "3596          WILKES-BARRE  18702  \n",
      "4264                 LIHUE  96766  \n",
      "4308    SAULT SAINTE MARIE  49783  \n",
      "4544      DISTRICT HEIGHTS  20747  \n",
      "4592      SOUTH SAINT PAUL  55075  \n",
      "4784              O FALLON  62269  \n",
      "5167          ST JOHNSBURY   5819  \n",
      "5254                 LIHUE  96766  \n",
      "6265          WILKES-BARRE  18701  \n",
      "6791    SAULT SAINTE MARIE  49783  \n",
      "6939         WINSTON-SALEM  27101  \n",
      "6991         WINSTON-SALEM  27103  \n",
      "7040    SAULT SAINTE MARIE  49783  \n",
      "7762        MOUNT PLEASANT  29464  \n",
      "7853          WILKES-BARRE  18702  \n",
      "7979    SAULT SAINTE MARIE  49783  \n",
      "8044          ST JOHNSBURY   5819  \n",
      "8093                 LIHUE  96766  \n",
      "8194    SAULT SAINTE MARIE  49783  \n",
      "8249    HILTON HEAD ISLAND  29926  \n"
     ]
    }
   ],
   "source": [
    "conflicts = df[df[\"City_x\"].str.upper() != df[\"Final_City\"].str.upper()]\n",
    "print(\"Final remaining conflicts:\", len(conflicts))\n",
    "print(conflicts[[\"Location Code\", \"City_x\", \"City_y\", \"Final_City\", \"zip\"]].head(38))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aff9ae-26dd-4bd8-9847-66c5d37ca102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolveddddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042253e-5b15-4c82-9c9d-772b7bb04963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Load the cleaned merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6d5a9-b082-4171-ae9b-eb71793492a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing price_latest values: 616\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset after state conflict resolution\n",
    "df = pd.read_csv(\"conflict_resolved.csv\")\n",
    "\n",
    "# Check missing price_latest values\n",
    "missing_price = df['price_latest'].isna().sum()\n",
    "print(f\"Missing price_latest values: {missing_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba6b54-ab7f-40d7-acf0-7d941ac25edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd7152-04c9-483d-bc23-aae366f43dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Configuration + imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ec10a-1ce9-4b72-8662-50b3c284a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config & imports\n",
    "import pandas as pd, numpy as np, math, re\n",
    "from pathlib import Path\n",
    "\n",
    "# for nearest-neighbor (fast, uses haversine)\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# zip -> lat/lon lookup\n",
    "import pgeocode\n",
    "\n",
    "# optional for geocoding by address (if needed)\n",
    "# from geopy.geocoders import Nominatim\n",
    "\n",
    "# constants / parameters you can tune\n",
    "ASSETS_PATH = Path(\"conflict_resolved.csv\")\n",
    "DS2_PATH    = Path(\"rwap25_gis_dataset2.csv\")\n",
    "OUTPUT_PATH = Path(\"assets_nearest_regions.csv\")\n",
    "\n",
    "K_NEIGHBORS = 5      # how many nearby regions to fetch per asset\n",
    "MAX_DISTANCE_KM = 200  # optional cap for acceptable nearest region distance\n",
    "N_MONTHS_FOR_AVG = 3   # if you want to average last N months for smoothing\n",
    "DISTANCE_DECAY_K = 0.05 # example decay constant for exp(-k*dist) weighting (optional)\n",
    "\n",
    "# create pgeocode nomi\n",
    "nomi = pgeocode.Nominatim('us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6399ba-e79c-4b0f-aa74-23517a354b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Utility helpers: robust column finder, zip normalizer, distance helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fc8ec-f413-4dc5-aa69-569c9a2d718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_col(df, candidates):\n",
    "    \"\"\"Return first column name in df that case-insensitively matches any candidate string.\"\"\"\n",
    "    cols_low = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_low:\n",
    "            return cols_low[cand.lower()]\n",
    "    # fallback: try normalized match (remove non-alnum)\n",
    "    norm_to_col = {re.sub(r'[^a-z0-9]','',c.lower()): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r'[^a-z0-9]','',cand.lower())\n",
    "        if key in norm_to_col:\n",
    "            return norm_to_col[key]\n",
    "    return None\n",
    "\n",
    "def to_zip5(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip()\n",
    "    s = \"\".join(ch for ch in s if ch.isdigit())\n",
    "    if s == \"\": return np.nan\n",
    "    return s.zfill(5)[:5]\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    # all args arrays or scalars in degrees\n",
    "    R = 6371.0\n",
    "    lat1r = np.radians(lat1); lon1r = np.radians(lon1)\n",
    "    lat2r = np.radians(lat2); lon2r = np.radians(lon2)\n",
    "    dlat = lat2r - lat1r\n",
    "    dlon = lon2r - lon1r\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1r)*np.cos(lat2r)*np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf4373-36b5-4c77-ba88-b30d2e36a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load datasets and detect relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674f16b-5145-4738-933b-399a311fcc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found columns in assets: {'loccode': 'location code', 'zip': 'zip', 'lat': 'latitude', 'lon': 'longitude', 'city': 'city', 'state': 'unified_state'}\n",
      "Found columns in ds2: {'regionname': 'RegionName', 'regionid': 'RegionID', 'regiontype': 'RegionType', 'city': 'City', 'metro': 'Metro', 'county': 'CountyName', 'state': 'State'}\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "assets = pd.read_csv(ASSETS_PATH)\n",
    "ds2    = pd.read_csv(DS2_PATH)\n",
    "\n",
    "# find important columns in assets (flexible names)\n",
    "col_loccode = guess_col(assets, [\"Location Code\", \"location code\", \"locationcode\", \"loccode\"])\n",
    "col_zip     = guess_col(assets, [\"zip\",\"Zip Code\",\"Zip\",\"zip_code\"])\n",
    "col_lat     = guess_col(assets, [\"Latitude\",\"lat\",\"latitude\"])\n",
    "col_lon     = guess_col(assets, [\"Longitude\",\"lon\",\"longitude\"])\n",
    "col_city    = guess_col(assets, [\"Final_City\",\"final_city\",\"City\",\"city\",\"City_x\",\"City_y\"])\n",
    "col_state   = guess_col(assets, [\"Unified_State\",\"unified_state\",\"State\",\"state\"])\n",
    "col_area    = guess_col(assets, [\"Building Rentable Square Feet\",\"Building Rentable Sq Ft\",\"bldg_sf\",\"Building Rentable Square Feet\"])\n",
    "\n",
    "print(\"Found columns in assets:\", dict(loccode=col_loccode, zip=col_zip, lat=col_lat, lon=col_lon, city=col_city, state=col_state))\n",
    "\n",
    "# find columns in ds2\n",
    "col_regionname = guess_col(ds2, [\"RegionName\",\"Region Name\",\"regionname\"])\n",
    "col_regionid   = guess_col(ds2, [\"RegionID\",\"Region Id\",\"regionid\"])  # Corrected the typo here\n",
    "col_regiontype = guess_col(ds2, [\"RegionType\",\"regiontype\"])\n",
    "col_ds2_city   = guess_col(ds2, [\"City\",\"city\"])\n",
    "col_ds2_metro  = guess_col(ds2, [\"Metro\",\"metro\"])\n",
    "col_ds2_county = guess_col(ds2, [\"CountyName\",\"County Name\",\"countyname\"])\n",
    "col_ds2_state  = guess_col(ds2, [\"State\",\"StateName\",\"state\",\"statename\"])\n",
    "\n",
    "print(\"Found columns in ds2:\", dict(regionname=col_regionname, regionid=col_regionid, regiontype=col_regiontype, city=col_ds2_city, metro=col_ds2_metro, county=col_ds2_county, state=col_ds2_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ccad8-17ef-4bc8-9cd7-902a939174e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Normalize ZIPs and get lat/lon for ds2 regions (use pgeocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a2f21-056d-44ab-9965-6b8073121ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions with coords: 26314 / 26314\n",
      "Assets with coords: 8652 / 8652\n"
     ]
    }
   ],
   "source": [
    "# standardize zip fields\n",
    "ds2['region_zip'] = ds2[col_regionname].apply(to_zip5)\n",
    "assets['zip_std'] = assets[col_zip].apply(to_zip5) if col_zip else np.nan\n",
    "\n",
    "# Try to use any lat/lon already present in ds2 (if columns exist)\n",
    "col_ds2_lat = guess_col(ds2, [\"Latitude\",\"lat\",\"latitude\"])\n",
    "col_ds2_lon = guess_col(ds2, [\"Longitude\",\"lon\",\"longitude\"])\n",
    "\n",
    "if col_ds2_lat and col_ds2_lon:\n",
    "    ds2['region_lat'] = pd.to_numeric(ds2[col_ds2_lat], errors='coerce')\n",
    "    ds2['region_lon'] = pd.to_numeric(ds2[col_ds2_lon], errors='coerce')\n",
    "else:\n",
    "    # use pgeocode to get lat/lon for each unique region_zip\n",
    "    unique_zips = ds2['region_zip'].dropna().unique().tolist()\n",
    "    rez = []\n",
    "    for z in unique_zips:\n",
    "        # pgeocode returns a pandas Series-like object; handle missing gracefully\n",
    "        r = nomi.query_postal_code(str(z))\n",
    "        lat = r.latitude if r is not None and not pd.isna(r.latitude) else np.nan\n",
    "        lon = r.longitude if r is not None and not pd.isna(r.longitude) else np.nan\n",
    "        rez.append((z, lat, lon))\n",
    "    zip_coord_df = pd.DataFrame(rez, columns=['region_zip','region_lat','region_lon'])\n",
    "    ds2 = ds2.merge(zip_coord_df, on='region_zip', how='left')\n",
    "\n",
    "# For assets: if lat/lon missing, fill by asset zip centroid (pgeocode)\n",
    "if not col_lat or not col_lon:\n",
    "    # create asset centroid from zip_std\n",
    "    unique_asset_zips = assets['zip_std'].dropna().unique().tolist()\n",
    "    rez2 = []\n",
    "    for z in unique_asset_zips:\n",
    "        r = nomi.query_postal_code(str(z))\n",
    "        lat = r.latitude if r is not None and not pd.isna(r.latitude) else np.nan\n",
    "        lon = r.longitude if r is not None and not pd.isna(r.longitude) else np.nan\n",
    "        rez2.append((z, lat, lon))\n",
    "    asset_zip_coords = pd.DataFrame(rez2, columns=['zip_std','zip_lat','zip_lon'])\n",
    "    # merge into assets\n",
    "    assets = assets.merge(asset_zip_coords, on='zip_std', how='left')\n",
    "    # create final lat/lon columns\n",
    "    assets['asset_lat'] = assets.get(col_lat).combine_first(assets['zip_lat']) if col_lat else assets['zip_lat']\n",
    "    assets['asset_lon'] = assets.get(col_lon).combine_first(assets['zip_lon']) if col_lon else assets['zip_lon']\n",
    "else:\n",
    "    assets['asset_lat'] = pd.to_numeric(assets[col_lat], errors='coerce')\n",
    "    assets['asset_lon'] = pd.to_numeric(assets[col_lon], errors='coerce')\n",
    "\n",
    "# final checks\n",
    "print(\"Regions with coords:\", ds2['region_lat'].notna().sum(), \"/\", len(ds2))\n",
    "print(\"Assets with coords:\", assets['asset_lat'].notna().sum(), \"/\", len(assets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b13174-116f-4fc2-9051-caba2574a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Identify monthly price columns in ds2 and select latest / N-month average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea90d9d-9a9f-44a5-b326-30bb920888a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest month column: 31-07-2025 2025-07-31\n"
     ]
    }
   ],
   "source": [
    "# detect date-like columns (dayfirst format dd-mm-YYYY or similar)\n",
    "date_cols = []\n",
    "for c in ds2.columns:\n",
    "    # try parse, dayfirst True\n",
    "    dt = pd.to_datetime(c, errors='coerce', dayfirst=True)\n",
    "    if pd.notna(dt):\n",
    "        date_cols.append((c, dt))\n",
    "date_cols = sorted(date_cols, key=lambda x: x[1])\n",
    "if not date_cols:\n",
    "    raise RuntimeError(\"No monthly price columns detected in dataset2. Check headers.\")\n",
    "\n",
    "latest_month_col = date_cols[-1][0]\n",
    "latest_month_date = date_cols[-1][1].date()\n",
    "print(\"Latest month column:\", latest_month_col, latest_month_date)\n",
    "\n",
    "# optional: compute last N months average for smoothing\n",
    "def compute_last_n_avg(df, n=N_MONTHS_FOR_AVG):\n",
    "    cols = [c for c,d in date_cols[-n:]]  # last n columns\n",
    "    return df[cols].mean(axis=1, skipna=True)\n",
    "\n",
    "ds2['latest_price'] = pd.to_numeric(ds2[latest_month_col], errors='coerce')\n",
    "# also add N-month avg\n",
    "ds2['latest_navg'] = compute_last_n_avg(ds2, n=N_MONTHS_FOR_AVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3735b-d95b-4c27-b369-63b84f214533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Build region-level (unique) table with coords + price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac034ce5-7f7b-4e91-97a4-839efadffe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region candidates: 26314\n"
     ]
    }
   ],
   "source": [
    "# build region table (unique RegionID or region_zip)\n",
    "region_cols = [col_regionid, col_regionname, 'region_zip', 'region_lat', 'region_lon', 'latest_price', 'latest_navg', col_ds2_metro, col_ds2_county, col_ds2_city, col_ds2_state]\n",
    "region_cols = [c for c in region_cols if c in ds2.columns or isinstance(c,str)]\n",
    "region_df = ds2.drop_duplicates(subset=['region_zip']).copy()\n",
    "region_df = region_df.loc[:, ['region_zip','region_lat','region_lon','latest_price','latest_navg', col_ds2_metro, col_ds2_county, col_ds2_city, col_ds2_state]]\n",
    "region_df = region_df.rename(columns={\n",
    "    col_ds2_metro: \"metro\",\n",
    "    col_ds2_county: \"county\",\n",
    "    col_ds2_city: \"city_ds2\",\n",
    "    col_ds2_state: \"state_ds2\"\n",
    "})\n",
    "# keep only rows that have coordinates and any price\n",
    "region_df = region_df[ region_df['region_lat'].notna() & region_df['region_lon'].notna() ]\n",
    "print(\"Region candidates:\", len(region_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db634b8c-e637-4369-b129-e6618d200740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Build BallTree (haversine) on region centroids and query K neighbors for assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a2ef7-8f5d-4afb-9438-084178947b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved neighbor table for 8652 assets.\n"
     ]
    }
   ],
   "source": [
    "# prepare arrays in radians\n",
    "region_coords_rad = np.radians(region_df[['region_lat','region_lon']].values)\n",
    "tree = BallTree(region_coords_rad, metric='haversine')  # distances in radians\n",
    "\n",
    "# assets query points (filter for those that have coords)\n",
    "assets_with_coords = assets[assets['asset_lat'].notna() & assets['asset_lon'].notna()].copy()\n",
    "assets_coords_rad = np.radians(assets_with_coords[['asset_lat','asset_lon']].values)\n",
    "\n",
    "# query\n",
    "dist_rad, idx = tree.query(assets_coords_rad, k=K_NEIGHBORS, return_distance=True)  # distances in radians\n",
    "dist_km = dist_rad * 6371.0  # convert to km\n",
    "\n",
    "# Build neighbor result table\n",
    "neigh_list = []\n",
    "for i_asset, asset_idx in enumerate(assets_with_coords.index):\n",
    "    asset_row = assets_with_coords.loc[asset_idx]\n",
    "    for j in range(idx.shape[1]):\n",
    "        reg_pos = idx[i_asset, j]\n",
    "        reg_row = region_df.iloc[reg_pos]\n",
    "        neigh_list.append({\n",
    "            'asset_index': asset_idx,\n",
    "            'location_code': asset_row.get(col_loccode),\n",
    "            'asset_lat': asset_row['asset_lat'],\n",
    "            'asset_lon': asset_row['asset_lon'],\n",
    "            'neighbor_rank': j+1,\n",
    "            'region_zip': reg_row['region_zip'],\n",
    "            'region_lat': reg_row['region_lat'],\n",
    "            'region_lon': reg_row['region_lon'],\n",
    "            'region_latest_price': reg_row['latest_price'],\n",
    "            'region_latest_navg': reg_row['latest_navg'],\n",
    "            'metro': reg_row.get('metro'),\n",
    "            'county': reg_row.get('county'),\n",
    "            'city_ds2': reg_row.get('city_ds2'),\n",
    "            'state_ds2': reg_row.get('state_ds2'),\n",
    "            'dist_km': float(dist_km[i_asset, j])\n",
    "        })\n",
    "\n",
    "neighbors_df = pd.DataFrame(neigh_list)\n",
    "# save neighbors table\n",
    "neighbors_df.to_csv(\"assets_nearest_regions_raw.csv\", index=False)\n",
    "print(\"Saved neighbor table for\", neighbors_df['location_code'].nunique(), \"assets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393eb9e-62a0-42c9-8300-e9ff41348584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Example functions to compute distance-weighted valuation (you can choose one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba9dd6-69a6-4d07-a77f-256b0f7d708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idw_prediction(neighs, price_col='region_latest_price', eps=1e-6):\n",
    "    \"\"\"Inverse-distance weighting: weights = 1 / (dist + eps).\"\"\"\n",
    "    vals = []\n",
    "    for asset, group in neighs.groupby('asset_index'):\n",
    "        group = group.dropna(subset=[price_col])\n",
    "        if group.empty:\n",
    "            vals.append((asset, np.nan))\n",
    "            continue\n",
    "        w = 1.0 / (group['dist_km'] + eps)\n",
    "        pred = (w * group[price_col]).sum() / w.sum()\n",
    "        vals.append((asset, pred))\n",
    "    return pd.DataFrame(vals, columns=['asset_index','pred_idw'])\n",
    "\n",
    "def exp_decay_prediction(neighs, price_col='region_latest_price', k=DISTANCE_DECAY_K):\n",
    "    \"\"\"Exponential decay weights: w = exp(-k * dist_km)\"\"\"\n",
    "    vals = []\n",
    "    for asset, group in neighs.groupby('asset_index'):\n",
    "        group = group.dropna(subset=[price_col])\n",
    "        if group.empty:\n",
    "            vals.append((asset, np.nan))\n",
    "            continue\n",
    "        w = np.exp(-k * group['dist_km'])\n",
    "        pred = (w * group[price_col]).sum() / w.sum()\n",
    "        vals.append((asset, pred))\n",
    "    return pd.DataFrame(vals, columns=['asset_index','pred_expdecay'])\n",
    "\n",
    "# Example usage (commented out; run when ready):\n",
    "# pred_idw = idw_prediction(neighbors_df, price_col='region_latest_price')\n",
    "# pred_exp = exp_decay_prediction(neighbors_df, price_col='region_latest_navg')\n",
    "# then merge preds back into assets_with_coords by index to get predicted prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8423779-1c13-4232-acd3-600399d4db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Merge neighbor info back to assets and save final mapping (no valuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b7f4e-8c0f-486e-8abe-3b4886cde8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved assets → nearest region mapping to: assets_nearest_regions.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get only the nearest neighbor (rank=1)\n",
    "nearest_one = neighbors_df[neighbors_df['neighbor_rank'] == 1].copy()\n",
    "\n",
    "# Rename region-related columns for clarity\n",
    "nearest_one = nearest_one.rename(columns={\n",
    "    'region_zip': 'nearest_region_zip',\n",
    "    'region_lat': 'nearest_region_lat',\n",
    "    'region_lon': 'nearest_region_lon',\n",
    "    'region_latest_price': 'nearest_region_price',\n",
    "    'region_latest_navg': 'nearest_region_price_navg',\n",
    "    'metro': 'nearest_region_metro',\n",
    "    'county': 'nearest_region_county',\n",
    "    'city_ds2': 'nearest_region_city',\n",
    "    'state_ds2': 'nearest_region_state',\n",
    "    'dist_km': 'nearest_dist_km'\n",
    "})\n",
    "\n",
    "# Step 2: Ensure we have asset_index in assets_with_coords\n",
    "assets_with_coords = assets_with_coords.reset_index().rename(columns={'index': 'asset_index'})\n",
    "\n",
    "# Step 3: Check if nearest_one has asset_index column\n",
    "if 'asset_index' not in nearest_one.columns:\n",
    "    # If not, try renaming from the column name in neighbors_df\n",
    "    if 'asset_id' in nearest_one.columns:\n",
    "        nearest_one = nearest_one.rename(columns={'asset_id': 'asset_index'})\n",
    "    elif 'index' in nearest_one.columns:\n",
    "        nearest_one = nearest_one.rename(columns={'index': 'asset_index'})\n",
    "    else:\n",
    "        print(\"⚠️ 'asset_index' missing in nearest_one, check neighbors_df columns!\")\n",
    "        print(\"Available columns:\", nearest_one.columns)\n",
    "\n",
    "# Step 4: Merge assets with nearest region info\n",
    "assets_mapped = assets_with_coords.merge(\n",
    "    nearest_one[[\n",
    "        'asset_index', 'nearest_region_zip', 'nearest_region_lat', 'nearest_region_lon',\n",
    "        'nearest_region_price', 'nearest_region_price_navg', 'nearest_region_metro',\n",
    "        'nearest_region_county', 'nearest_region_city', 'nearest_region_state',\n",
    "        'nearest_dist_km'\n",
    "    ]],\n",
    "    on='asset_index',  # Use a single 'on' since both have the same column name now\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 5: Save final mapping (assets with nearest region info)\n",
    "assets_mapped.to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"✅ Saved assets → nearest region mapping to:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d2f0e-e8c1-46ef-aee5-abe0736166d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assets columns: Index(['asset_index', 'asset_index_original', 'location code',\n",
      "       'real property asset name', 'installation name', 'owned or leased',\n",
      "       'gsa region', 'street address', 'zip code', 'latitude', 'longitude',\n",
      "       'building rentable square feet', 'available square feet',\n",
      "       'construction date', 'congressional district',\n",
      "       'congressional district representative name', 'building status',\n",
      "       'real property asset type', 'zip_raw', 'zip', 'regionid', 'countyname',\n",
      "       'price_latest', 'price_latest_date', 'source', 'unified_state',\n",
      "       'state_source', 'city', 'zip_std', 'asset_lat', 'asset_lon'],\n",
      "      dtype='object')\n",
      "Nearest one columns: Index(['asset_index', 'location_code', 'asset_lat', 'asset_lon',\n",
      "       'neighbor_rank', 'nearest_region_zip', 'nearest_region_lat',\n",
      "       'nearest_region_lon', 'nearest_region_price',\n",
      "       'nearest_region_price_navg', 'nearest_region_metro',\n",
      "       'nearest_region_county', 'nearest_region_city', 'nearest_region_state',\n",
      "       'nearest_dist_km'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Assets columns:\", assets_with_coords.columns)\n",
    "print(\"Nearest one columns:\", nearest_one.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb1c36-8b7a-4eba-8b96-642800a0596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Optional: Visualization (Folium) — show assets colored by distance or nearest metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cc94e-09f7-4bb4-bc34-34ef64389dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved interactive map: assets_nearest_map.html\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: visualize subset to sanity-check mapping (requires folium)\n",
    "import folium\n",
    "m = folium.Map(location=[assets_mapped['asset_lat'].mean(), assets_mapped['asset_lon'].mean()], zoom_start=6)\n",
    "# add nearest region markers\n",
    "for _, r in assets_mapped[['asset_lat','asset_lon','nearest_region_zip','nearest_region_price','nearest_dist_km']].dropna().head(500).iterrows():\n",
    "    folium.CircleMarker(location=[r['asset_lat'], r['asset_lon']], radius=3,\n",
    "                        popup=f\"zip:{r['nearest_region_zip']} dist_km:{r['nearest_dist_km']:.1f} price:{r['nearest_region_price']}\",\n",
    "                        fill=True).add_to(m)\n",
    "m.save(\"assets_nearest_map.html\")\n",
    "print(\"Saved interactive map: assets_nearest_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb7991-8ee8-4295-8db8-bb91e78bb7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5f5e4-a830-4ccb-86a4-be4b0be76274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759c953-051e-4d16-af1d-7bc85ab5727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's define the function to calculate price based on distance\n",
    "def estimate_price(nearest_price, distance_km, decay='inverse', k=0.1):\n",
    "    \"\"\"\n",
    "    Estimate asset price based on nearest region price and distance.\n",
    "    \n",
    "    nearest_price: float\n",
    "    distance_km: float\n",
    "    decay: 'inverse' | 'exponential' | 'linear'\n",
    "    k: decay constant (only for exponential or linear)\n",
    "    \"\"\"\n",
    "    if np.isnan(nearest_price):\n",
    "        return np.nan\n",
    "    \n",
    "    if decay == 'inverse':\n",
    "        weight = 1 / (1 + distance_km)\n",
    "    elif decay == 'exponential':\n",
    "        weight = np.exp(-k * distance_km)\n",
    "    elif decay == 'linear':\n",
    "        weight = max(0, 1 - k * distance_km)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid decay type. Choose from 'inverse', 'exponential', or 'linear'\")\n",
    "        \n",
    "    return nearest_price * weight\n",
    "\n",
    "# Apply valuation\n",
    "assets_mapped['estimated_price'] = assets_mapped.apply(\n",
    "    lambda row: estimate_price(row['nearest_region_price'], row['nearest_dist_km'], decay='inverse'),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bf72b-c76a-4b2a-b03d-01839a2ddaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Valuation completed and saved.\n"
     ]
    }
   ],
   "source": [
    "assets_mapped.to_csv(\"final_asset_valuation.csv\", index=False)\n",
    "print(\"✅ Valuation completed and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
