{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb1b288-eca3-4dc7-b2ec-0ad5f8c2342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df64c448-608b-4d5b-b41d-4af19df5c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e073285c-5e91-460e-bed8-8276af501043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — load the two CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83bd69b-62ef-471a-9582-512b6c023574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF1 rows,cols: (8652, 18)\n",
      "DF2 rows,cols: (26314, 316)\n",
      "DF1 columns: ['Location Code', 'Real Property Asset Name', 'Installation Name', 'Owned or Leased', 'GSA Region', 'Street Address', 'City', 'State', 'Zip Code', 'Latitude', 'Longitude', 'Building Rentable Square Feet', 'Available Square Feet', 'Construction Date', 'Congressional District', 'Congressional District Representative Name', 'Building Status', 'Real Property Asset Type']\n",
      "DF2 columns (sample): ['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', 'State', 'City', 'Metro', 'CountyName', '31-01-2000', '29-02-2000', '31-03-2000']\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('rwap25_gis_dataset1.csv')   # assets\n",
    "df2 = pd.read_csv('rwap25_gis_dataset2.csv')   # zip time-series\n",
    "\n",
    "print(\"DF1 rows,cols:\", df1.shape)\n",
    "print(\"DF2 rows,cols:\", df2.shape)\n",
    "print(\"DF1 columns:\", df1.columns.tolist())\n",
    "print(\"DF2 columns (sample):\", df2.columns[:12].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6f2833-8415-464e-9ab3-c764862a4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — keep only zip-level rows from Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b69f4c0-a2d6-4163-9807-ad1db61bc1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered df2 to zip rows: (26314, 316)\n"
     ]
    }
   ],
   "source": [
    "# If RegionType exists, filter rows where it's 'zip' (case-insensitive).\n",
    "if 'RegionType' in df2.columns:\n",
    "    df2_zip = df2[df2['RegionType'].astype(str).str.lower() == 'zip'].copy()\n",
    "else:\n",
    "    df2_zip = df2.copy()\n",
    "\n",
    "print(\"Filtered df2 to zip rows:\", df2_zip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6360af3e-a50b-43f1-8a77-a7cbca939742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — standardize ZIP strings in both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd116ba5-e116-4c78-bf4c-7ef95664746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique zips in df1: 3581\n",
      "Unique zips in df2_zip: 26314\n"
     ]
    }
   ],
   "source": [
    "# For df1: create a standardized 'zip' column from 'Zip Code'\n",
    "if 'Zip Code' in df1.columns:\n",
    "    df1['Zip_raw'] = df1['Zip Code'].astype(str).str.strip().fillna('')\n",
    "else:\n",
    "    # fallback: take first column name that contains 'zip'\n",
    "    zip_col = [c for c in df1.columns if 'zip' in c.lower()]\n",
    "    df1['Zip_raw'] = df1[zip_col[0]].astype(str).str.strip() if zip_col else ''\n",
    "\n",
    "df1['zip'] = df1['Zip_raw'].str.extract(r'(\\d+)', expand=False).fillna('').str.zfill(5)\n",
    "\n",
    "# For df2_zip: extract numeric from RegionName -> zip\n",
    "if 'RegionName' in df2_zip.columns:\n",
    "    df2_zip['RegionName_raw'] = df2_zip['RegionName'].astype(str).str.strip().fillna('')\n",
    "    df2_zip['zip'] = df2_zip['RegionName_raw'].str.extract(r'(\\d+)', expand=False).fillna('').str.zfill(5)\n",
    "else:\n",
    "    # fallback: find any column that looks like RegionName\n",
    "    cand = [c for c in df2_zip.columns if 'region' in c.lower() or 'zip' in c.lower()]\n",
    "    if cand:\n",
    "        df2_zip['RegionName_raw'] = df2_zip[cand[0]].astype(str).str.strip().fillna('')\n",
    "        df2_zip['zip'] = df2_zip['RegionName_raw'].str.extract(r'(\\d+)', expand=False).fillna('').str.zfill(5)\n",
    "    else:\n",
    "        df2_zip['RegionName_raw'] = ''\n",
    "        df2_zip['zip'] = ''\n",
    "\n",
    "print(\"Unique zips in df1:\", df1['zip'].nunique())\n",
    "print(\"Unique zips in df2_zip:\", df2_zip['zip'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be8457c9-f3ae-42ac-a0d8-9c7ab3c54e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 — find the date columns in Dataset 2 and melt it to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4f2706-d425-4858-b7c8-e8c1b5c9750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found date columns count: 307\n",
      "Sample date columns: ['31-01-2000', '29-02-2000', '31-03-2000', '30-04-2000', '31-05-2000', '30-06-2000']\n",
      "df2_long rows: (8078398, 15)\n"
     ]
    }
   ],
   "source": [
    "# find columns matching dd-mm-yyyy (e.g. '31-07-2025')\n",
    "date_pattern = re.compile(r'^\\d{2}-\\d{2}-\\d{4}$')\n",
    "date_cols = [c for c in df2_zip.columns if date_pattern.match(c)]\n",
    "\n",
    "print(\"Found date columns count:\", len(date_cols))\n",
    "print(\"Sample date columns:\", date_cols[:6])\n",
    "\n",
    "# melt wide->long: id_vars are everything except date columns\n",
    "id_vars = [c for c in df2_zip.columns if c not in date_cols]\n",
    "df2_long = df2_zip.melt(id_vars=id_vars, value_vars=date_cols,\n",
    "                        var_name='date_str', value_name='price_raw')\n",
    "\n",
    "# parse date and numeric price\n",
    "df2_long['date'] = pd.to_datetime(df2_long['date_str'], format='%d-%m-%Y', errors='coerce')\n",
    "df2_long['price'] = pd.to_numeric(df2_long['price_raw'], errors='coerce')\n",
    "\n",
    "# drop invalid date rows (if any)\n",
    "df2_long = df2_long.dropna(subset=['date']).copy()\n",
    "\n",
    "print(\"df2_long rows:\", df2_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8921e56-aea9-4e04-ad28-4b2242e8dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 — compute the latest non-null price and its date per ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e9c9a58-5f54-41b6-8c9c-898c0d5bc03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zips with latest price available: 26314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>price_latest</th>\n",
       "      <th>price_latest_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>340459.1165</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01002</td>\n",
       "      <td>538321.8056</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01005</td>\n",
       "      <td>409267.9918</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01007</td>\n",
       "      <td>467503.3545</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01008</td>\n",
       "      <td>372725.8783</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     zip  price_latest price_latest_date\n",
       "0  01001   340459.1165        2025-07-31\n",
       "1  01002   538321.8056        2025-07-31\n",
       "2  01005   409267.9918        2025-07-31\n",
       "3  01007   467503.3545        2025-07-31\n",
       "4  01008   372725.8783        2025-07-31"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only rows where price is not null, then take the last (latest date) per zip\n",
    "df2_long_nonnull = df2_long.dropna(subset=['price']).copy()\n",
    "\n",
    "# if there are no non-null prices we'll get an empty df\n",
    "if df2_long_nonnull.empty:\n",
    "    print(\"Warning: dataset2 has no non-null price values.\")\n",
    "    latest_per_zip = pd.DataFrame(columns=['zip','price_latest','price_latest_date'])\n",
    "else:\n",
    "    latest_per_zip = (df2_long_nonnull.sort_values(['zip','date'])\n",
    "                                  .groupby('zip', as_index=False)\n",
    "                                  .last()[['zip','price','date']])\n",
    "    latest_per_zip = latest_per_zip.rename(columns={'price':'price_latest','date':'price_latest_date'})\n",
    "\n",
    "print(\"Zips with latest price available:\", len(latest_per_zip))\n",
    "latest_per_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bdf4e0d-6c28-4dfa-9995-51a2a6e4c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 — merge latest-per-zip info back with meta (City/State) and attach to Dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "610328b2-4568-4f32-b81a-9cc1bcf4faed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged rows: (8652, 27)\n",
      "Fraction with price_latest: 0.9288025889967637\n"
     ]
    }
   ],
   "source": [
    "# pick some metadata from df2_zip (if present)\n",
    "meta_cols = []\n",
    "for c in ['RegionID','City','State','CountyName']:\n",
    "    if c in df2_zip.columns:\n",
    "        meta_cols.append(c)\n",
    "meta_cols = list(dict.fromkeys(meta_cols))  # keep order & unique\n",
    "\n",
    "zip_meta = df2_zip[meta_cols + ['zip']].drop_duplicates(subset=['zip'], keep='first')\n",
    "\n",
    "# create zip_info: meta + latest price\n",
    "zip_info = zip_meta.merge(latest_per_zip, on='zip', how='left')\n",
    "\n",
    "# left-merge into df1 (many assets per zip -> one zip_info row)\n",
    "merged = df1.merge(zip_info, on='zip', how='left', validate='m:1')\n",
    "\n",
    "# initial source flag: exact_zip if we have a price_latest\n",
    "# Ensure the 'source' column is of type object to handle mixed types\n",
    "merged['source'] = np.where(merged['price_latest'].notna(), 'exact_zip', None)\n",
    "\n",
    "print(\"Merged rows:\", merged.shape)\n",
    "print(\"Fraction with price_latest:\", merged['price_latest'].notna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47ad7c9c-e17d-415e-9934-ca95f14298dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 — inspect assets that don’t have price_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7305a092-220f-4c22-98fe-8cb80b7c1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of assets missing price_latest: 616\n",
      "Sample missing zips (up to 50): ['20993', '47907', '85620', '80225', '00968', '70803', '59482', '78567', '20192', '59256', '88029', '83853', '24011', '96799', '99752', '59542', '92283', '05460', '76155', '08608', '97204', '59411', '04491', '10278', '52801', '20373', '99780', '58329', '00716', '00708', '20585', '00820', '04936', '00641', '20503', '00784', '85633', '10038', '79711', '14604', '77010', '63145', '87026', '20201', '79839', '47405', '96950', '00918', '78235', '00824']\n",
      "Empty DataFrame\n",
      "Columns: [zip, n_obs, n_non_null, first_date, last_date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "missing = merged[merged['price_latest'].isna()].copy()\n",
    "print(\"Number of assets missing price_latest:\", len(missing))\n",
    "# show unique missing zips and their df2_long summary\n",
    "missing_zips = missing['zip'].unique().tolist()[:50]\n",
    "print(\"Sample missing zips (up to 50):\", missing_zips)\n",
    "\n",
    "# For diagnostics: show df2_long info for those zips\n",
    "diag = (df2_long[df2_long['zip'].isin(missing_zips)]\n",
    "        .groupby('zip').agg(n_obs=('price','size'),\n",
    "                            n_non_null=('price', lambda s: s.notna().sum()),\n",
    "                            first_date=('date','min'),\n",
    "                            last_date=('date','max')).reset_index())\n",
    "print(diag.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08acdc4-c0d7-4339-a3e4-83cf772a8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 — save the merged file (with source) so you have a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "156d9b8f-66b8-4193-b472-63296b8be3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged file to dataset1_merged_step.csv\n"
     ]
    }
   ],
   "source": [
    "merged.to_csv('dataset1_merged_step.csv', index=False)\n",
    "print(\"Saved merged file to dataset1_merged_step.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ce091-a5e5-4852-a836-6c221218db60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8e0e7-600c-468d-b839-d0568775bd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b85dc4-6ae5-439a-84f5-690f2ea1e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP A — quick read/setup (if not done already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b92d366b-3c57-453d-b596-437563632c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "merged = pd.read_csv('dataset1_merged_step.csv')  # file from earlier step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bc4e7-1900-4f7e-8004-e14ac32a4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a36e5-c9e6-4efa-945d-fefa549c4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspect (see what's filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "412a80f5-004d-4359-9963-84d2040eefb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-null counts:\n",
      "State_x: 8652 / 8652\n",
      "State_y: 8036 / 8652\n",
      "\n",
      "Sample pairs (first 20 rows):\n",
      "   State_x State_y\n",
      "0       GA      GA\n",
      "1       WI      WI\n",
      "2       MN      MN\n",
      "3       MD      MD\n",
      "4       CO      CO\n",
      "5       CO      CO\n",
      "6       FL      FL\n",
      "7       AZ      AZ\n",
      "8       FL      FL\n",
      "9       MD     NaN\n",
      "10      CA      CA\n",
      "11      CA      CA\n",
      "12      IN     NaN\n",
      "13      AZ      AZ\n",
      "14      PA      PA\n",
      "15      UT      UT\n",
      "16      FL      FL\n",
      "17      MD      MD\n",
      "18      PA      PA\n",
      "19      AZ      AZ\n"
     ]
    }
   ],
   "source": [
    "# show counts and a sample\n",
    "print(\"Non-null counts:\")\n",
    "print(\"State_x:\", merged['State_x'].notna().sum(), \"/\", len(merged))\n",
    "print(\"State_y:\", merged['State_y'].notna().sum(), \"/\", len(merged))\n",
    "\n",
    "print(\"\\nSample pairs (first 20 rows):\")\n",
    "print(merged[['State_x','State_y']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440111b-290e-4aa6-b646-1af4ff69c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1 — Create unified State and state_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c485568-0bfd-439c-b8ed-0c196e7310d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified State non-null: 8652 / 8652\n",
      "state_source counts:\n",
      " state_source\n",
      "both_same        8035\n",
      "state_x           616\n",
      "both_conflict       1\n",
      "Name: count, dtype: int64\n",
      "Sample unified states: ['GA' 'WI' 'MN' 'MD' 'CO' 'FL' 'AZ' 'CA' 'IN' 'PA' 'UT' 'TX' 'MO' 'NM'\n",
      " 'OR' 'VA' 'WY' 'LA' 'KY' 'MI']\n"
     ]
    }
   ],
   "source": [
    "# run this first (assumes merged DataFrame exists)\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "# prefer State_x, else State_y\n",
    "merged['State_unified_raw'] = merged['State_x'].where(\n",
    "    merged['State_x'].notna() & (merged['State_x'].astype(str).str.strip()!=''),\n",
    "    merged['State_y']\n",
    ")\n",
    "\n",
    "# normalize whitespace and uppercase, convert empty/'nan' strings to None\n",
    "merged['State_unified_raw'] = merged['State_unified_raw'].astype(str).str.strip().replace({'': None, 'nan': None, 'None': None})\n",
    "merged['State_unified_raw'] = merged['State_unified_raw'].where(merged['State_unified_raw'].notna(), None)\n",
    "merged['State_unified_raw'] = merged['State_unified_raw'].apply(lambda x: x.upper() if pd.notna(x) else x)\n",
    "\n",
    "# simple full-name -> abbrev mapping (extend if needed)\n",
    "state_map = {\n",
    "    'ALABAMA':'AL','ALASKA':'AK','ARIZONA':'AZ','ARKANSAS':'AR','CALIFORNIA':'CA',\n",
    "    'COLORADO':'CO','CONNECTICUT':'CT','DELAWARE':'DE','FLORIDA':'FL','GEORGIA':'GA',\n",
    "    'HAWAII':'HI','IDAHO':'ID','ILLINOIS':'IL','INDIANA':'IN','IOWA':'IA','KANSAS':'KS',\n",
    "    'KENTUCKY':'KY','LOUISIANA':'LA','MAINE':'ME','MARYLAND':'MD','MASSACHUSETTS':'MA',\n",
    "    'MICHIGAN':'MI','MINNESOTA':'MN','MISSISSIPPI':'MS','MISSOURI':'MO','MONTANA':'MT',\n",
    "    'NEBRASKA':'NE','NEVADA':'NV','NEW HAMPSHIRE':'NH','NEW JERSEY':'NJ','NEW MEXICO':'NM',\n",
    "    'NEW YORK':'NY','NORTH CAROLINA':'NC','NORTH DAKOTA':'ND','OHIO':'OH','OKLAHOMA':'OK',\n",
    "    'OREGON':'OR','PENNSYLVANIA':'PA','RHODE ISLAND':'RI','SOUTH CAROLINA':'SC',\n",
    "    'SOUTH DAKOTA':'SD','TENNESSEE':'TN','TEXAS':'TX','UTAH':'UT','VERMONT':'VT',\n",
    "    'VIRGINIA':'VA','WASHINGTON':'WA','WEST VIRGINIA':'WV','WISCONSIN':'WI','WYOMING':'WY',\n",
    "    'DISTRICT OF COLUMBIA':'DC', 'DC':'DC'\n",
    "}\n",
    "\n",
    "def to_state_abbrev(s):\n",
    "    if s is None: return None\n",
    "    s_up = str(s).strip().upper()\n",
    "    if len(s_up) == 2 and s_up.isalpha(): \n",
    "        return s_up\n",
    "    return state_map.get(s_up, s_up)  # fallback keep as-is for manual inspection\n",
    "\n",
    "merged['State'] = merged['State_unified_raw'].apply(to_state_abbrev)\n",
    "\n",
    "# mark source\n",
    "def state_source(row):\n",
    "    sx = row.get('State_x')\n",
    "    sy = row.get('State_y')\n",
    "    if pd.notna(sx) and str(sx).strip()!='' and pd.notna(sy) and str(sy).strip()!='':\n",
    "        if str(sx).strip().upper() == str(sy).strip().upper():\n",
    "            return 'both_same'\n",
    "        else:\n",
    "            return 'both_conflict'\n",
    "    if pd.notna(sx) and str(sx).strip()!='':\n",
    "        return 'state_x'\n",
    "    if pd.notna(sy) and str(sy).strip()!='':\n",
    "        return 'state_y'\n",
    "    return 'none'\n",
    "\n",
    "merged['state_source'] = merged.apply(state_source, axis=1)\n",
    "\n",
    "# Quick check\n",
    "print(\"Unified State non-null:\", merged['State'].notna().sum(), \"/\", len(merged))\n",
    "print(\"state_source counts:\\n\", merged['state_source'].value_counts())\n",
    "print(\"Sample unified states:\", merged['State'].unique()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6462b535-3855-48a5-a73b-12e752396a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the one conflict and resolve it (quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccfc2e8f-230f-4251-846b-dddc0fe2b262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conflicts: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location Code</th>\n",
       "      <th>State_x</th>\n",
       "      <th>State_y</th>\n",
       "      <th>City_x</th>\n",
       "      <th>City_y</th>\n",
       "      <th>zip</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>NJ0148</td>\n",
       "      <td>NJ</td>\n",
       "      <td>NY</td>\n",
       "      <td>NEWARK</td>\n",
       "      <td>New Windsor</td>\n",
       "      <td>12553</td>\n",
       "      <td>40.728366</td>\n",
       "      <td>-74.174679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Location Code State_x State_y  City_x       City_y    zip   Latitude  \\\n",
       "1497        NJ0148      NJ      NY  NEWARK  New Windsor  12553  40.728366   \n",
       "\n",
       "      Longitude  \n",
       "1497 -74.174679  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show conflicting rows where both exist but are different\n",
    "conflicts = merged[(merged['State_x'].notna()) & (merged['State_y'].notna()) &\n",
    "                   (merged['State_x'].astype(str).str.strip().str.upper() != merged['State_y'].astype(str).str.strip().str.upper())]\n",
    "\n",
    "print(\"Number of conflicts:\", len(conflicts))\n",
    "conflicts[['Location Code','State_x','State_y','City_x','City_y','zip','Latitude','Longitude']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af413276-1ac5-46b3-9ce4-be552c50db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solving the conflict - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff813885-b96b-49c5-a4ab-1dbddbe9b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install & import (only if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f59a4a4-4288-4f98-babf-7a9c79a30254",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pgeocode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpgeocode\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pgeocode'"
     ]
    }
   ],
   "source": [
    "import pgeocode\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83452a6c-4090-4047-84aa-3eaff4688fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ZIP-to-state lookup to get authoritative state code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86802a-30c3-42f9-a940-9297a4b980fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP 12553 -> NY\n"
     ]
    }
   ],
   "source": [
    "# create pgeocode nominatim object for US\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "# function to lookup 2-letter state by zip (returns None if not found)\n",
    "def zip_to_state_abbrev(zipcode):\n",
    "    try:\n",
    "        info = nomi.query_postal_code(str(zipcode).zfill(5))\n",
    "        # pgeocode returns NaN for missing fields; state_code property is abbreviation if present\n",
    "        st = info['state_code']\n",
    "        if pd.isna(st):\n",
    "            return None\n",
    "        return str(st).strip().upper()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# test on the conflict zip (12553)\n",
    "print(\"ZIP 12553 ->\", zip_to_state_abbrev('12553'))  # expected 'NY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde0efc-0472-4e4b-bfc9-12c404e560c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve all conflicts using the ZIP lookup and record provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406cbff6-a65e-4086-9ce7-ac4640b6ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply zip lookup for all conflicted rows and update the merged DataFrame\n",
    "for idx, row in conflicts.iterrows():\n",
    "    z = row['zip']\n",
    "    authoritative_state = zip_to_state_abbrev(z)\n",
    "    if authoritative_state:\n",
    "        # keep original values for audit\n",
    "        merged.at[idx, 'State_before_resolution'] = merged.at[idx, 'State']  # current unified state\n",
    "        merged.at[idx, 'State_resolved_by_zip'] = authoritative_state\n",
    "        # update unified State to authoritative zip result\n",
    "        merged.at[idx, 'State'] = authoritative_state\n",
    "        merged.at[idx, 'state_source'] = 'zip_lookup_override'\n",
    "    else:\n",
    "        # if the lookup failed, leave current State as-is but flag for manual review\n",
    "        merged.at[idx, 'State_resolve_note'] = 'zip_lookup_failed_manual_review'\n",
    "        merged.at[idx, 'state_source'] = 'conflict_needs_manual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831474e-0c97-4bcd-a0f8-2f56c3f7c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audit the change (show rows that were modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9344ba4-5b53-4666-88c1-a4a535b9d152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved rows via ZIP lookup: 1\n",
      "     Location Code    zip State_x State_y State_before_resolution  \\\n",
      "1497        NJ0148  12553      NJ      NY                      NJ   \n",
      "\n",
      "     State_resolved_by_zip State         state_source  \n",
      "1497                    NY    NY  zip_lookup_override  \n"
     ]
    }
   ],
   "source": [
    "# show those rows you just modified\n",
    "resolved = merged[merged['state_source']=='zip_lookup_override']\n",
    "print(\"Resolved rows via ZIP lookup:\", len(resolved))\n",
    "print(resolved[['Location Code','zip','State_x','State_y','State_before_resolution','State_resolved_by_zip','State','state_source']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c25fd-2e7d-4071-ae30-95969711bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an audit log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f09446-49f5-42f0-b7f6-0395091a1b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state resolution audit to state_resolution_audit.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the conflict-resolution audit for traceability\n",
    "audit = merged.loc[conflicts.index, ['Location Code','zip','State_x','State_y','State_before_resolution','State_resolved_by_zip','State','state_source']]\n",
    "audit.to_csv('state_resolution_audit.csv', index=False)\n",
    "print(\"Saved state resolution audit to state_resolution_audit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c45953-aee8-4d4a-8a16-66be75ddeb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b4a8d-4e09-410c-9c9e-1ee1b6e5bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (8652, 27)\n",
      "Columns available: Index(['Location Code', 'Real Property Asset Name', 'Installation Name',\n",
      "       'Owned or Leased', 'GSA Region', 'Street Address', 'City_x', 'State_x',\n",
      "       'Zip Code', 'Latitude', 'Longitude', 'Building Rentable Square Feet',\n",
      "       'Available Square Feet', 'Construction Date', 'Congressional District',\n",
      "       'Congressional District Representative Name', 'Building Status',\n",
      "       'Real Property Asset Type', 'Zip_raw', 'zip', 'RegionID', 'City_y',\n",
      "       'State_y', 'CountyName', 'price_latest', 'price_latest_date', 'source'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged dataset (replace with your merged CSV file name)\n",
    "df = pd.read_csv(\"dataset1_merged_step.csv\")\n",
    "\n",
    "print(\"Initial shape:\", df.shape)\n",
    "print(\"Columns available:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd6d58-fc1a-4ce2-b8a9-c9ad92ff3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_state(row):\n",
    "    # If both match → use either\n",
    "    if pd.notna(row['State_x']) and pd.notna(row['State_y']):\n",
    "        if row['State_x'] == row['State_y']:\n",
    "            return row['State_x'], 'both_same'\n",
    "        else:\n",
    "            return row['State_x'], 'both_conflict'\n",
    "    # If only State_x exists\n",
    "    elif pd.notna(row['State_x']):\n",
    "        return row['State_x'], 'state_x'\n",
    "    # If only State_y exists\n",
    "    elif pd.notna(row['State_y']):\n",
    "        return row['State_y'], 'state_y'\n",
    "    # If both are null\n",
    "    else:\n",
    "        return None, 'missing'\n",
    "\n",
    "df[['Unified_State', 'state_source']] = df.apply(resolve_state, axis=1, result_type=\"expand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821991dc-bc22-43cf-9fc2-b0e89c18c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conflicts: 1\n",
      "     Location Code State_x State_y  City_x       City_y    zip\n",
      "1497        NJ0148      NJ      NY  NEWARK  New Windsor  12553\n"
     ]
    }
   ],
   "source": [
    "conflicts = df[df['state_source'] == 'both_conflict']\n",
    "print(f\"Number of conflicts: {len(conflicts)}\")\n",
    "\n",
    "if len(conflicts) > 0:\n",
    "    print(conflicts[['Location Code','State_x','State_y','City_x','City_y','zip']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d97eb-7581-4eb9-9856-26b3744483fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NJ0148, correct Unified_State to NJ\n",
    "df.loc[df['Location Code'] == 'NJ0148', 'Unified_State'] = 'NJ'\n",
    "df.loc[df['Location Code'] == 'NJ0148', 'state_source'] = 'resolved_conflict'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314452c-8a13-4e1a-a1a7-9eb0fac0f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['State_x', 'State_y'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e85c6-b259-40b8-b5af-6a6959559a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean dataset saved as: state_resolution_audit.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"state_resolution_audit.csv\", index=False)\n",
    "print(\"✅ Clean dataset saved as: state_resolution_audit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45103af8-5a55-47d0-9b49-71286a655c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unifying the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a6faa-8a72-4441-b836-da1406a80b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Load the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6884f-2a21-4ed6-bae7-a34296be6d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (8652, 27)\n",
      "Columns: Index(['Location Code', 'Real Property Asset Name', 'Installation Name',\n",
      "       'Owned or Leased', 'GSA Region', 'Street Address', 'City_x', 'Zip Code',\n",
      "       'Latitude', 'Longitude', 'Building Rentable Square Feet',\n",
      "       'Available Square Feet', 'Construction Date', 'Congressional District',\n",
      "       'Congressional District Representative Name', 'Building Status',\n",
      "       'Real Property Asset Type', 'Zip_raw', 'zip', 'RegionID', 'City_y',\n",
      "       'CountyName', 'price_latest', 'price_latest_date', 'source',\n",
      "       'Unified_State', 'state_source'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset from the previous step\n",
    "df = pd.read_csv(\"state_resolution_audit.csv\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f619b88-f645-4ecf-8330-5f7e7f72f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — Create Unified_City column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc3cb6-6c23-462f-a27e-9c4b192ba298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll define rules for merging City_x and City_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1d18f-cd74-4287-8972-ea3e60d837c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_city(row):\n",
    "    city_x = str(row['City_x']).strip() if pd.notna(row['City_x']) else None\n",
    "    city_y = str(row['City_y']).strip() if pd.notna(row['City_y']) else None\n",
    "\n",
    "    # If both are present and same → use either\n",
    "    if city_x and city_y:\n",
    "        if city_x.lower() == city_y.lower():  # case-insensitive match\n",
    "            return city_x, 'both_same'\n",
    "        else:\n",
    "            return city_x, 'both_conflict'  # temporarily mark conflict\n",
    "\n",
    "    # If only city_x exists\n",
    "    elif city_x:\n",
    "        return city_x, 'city_x'\n",
    "\n",
    "    # If only city_y exists\n",
    "    elif city_y:\n",
    "        return city_y, 'city_y'\n",
    "\n",
    "    # If both are missing\n",
    "    else:\n",
    "        return None, 'missing'\n",
    "\n",
    "# Apply function to dataframe\n",
    "df[['Unified_City', 'city_source']] = df.apply(resolve_city, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c42539-bef3-407c-bf28-58bd1753a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Check conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208c8a9-9e25-4234-a64d-e9828eaf64d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of city conflicts: 660\n",
      "    Location Code                 City_x              City_y    zip\n",
      "26         NM0576               PLACITAS        Santa Teresa  88008\n",
      "48         MI2137               MUSKEGON      Roosevelt Park  49441\n",
      "63         NY7402               BROOKLYN            New York  11201\n",
      "75         AR0062     HOT SPGS NATL PARK         Hot Springs  71901\n",
      "87         IL2520             BELLEVILLE              Shiloh  62221\n",
      "91         NC2314        WASHINGTON PARK          Washington  27889\n",
      "98         IL2491            SPRINGFIELD        Leland Grove  62704\n",
      "101        MO0617              ST. LOUIS         Saint Louis  63120\n",
      "128        MO0610              ST. LOUIS         Saint Louis  63120\n",
      "157        GA1158                ATLANTA            Chamblee  30341\n",
      "168        MI2187  CHESTERFIELD TOWNSHIP        Chesterfield  48051\n",
      "175        FL3183                  MIAMI       The Crossings  33186\n",
      "184        OH2418              BEACHWOOD      Shaker Heights  44122\n",
      "186        NJ5116              MT LAUREL  Mt Laurel Township   8054\n",
      "215        VT0756             NORTH TROY                Troy   5859\n",
      "225        MO1812              ST. LOUIS         Saint Louis  63101\n",
      "229        VA3056                HENRICO            Lakeside  23228\n",
      "234        NJ4525                MARLTON    Evesham Township   8053\n",
      "241        FL3206                  MIAMI       The Crossings  33186\n",
      "242        VT0757             NORTH TROY                Troy   5859\n"
     ]
    }
   ],
   "source": [
    "conflicts = df[df['city_source'] == 'both_conflict']\n",
    "print(f\"Number of city conflicts: {len(conflicts)}\")\n",
    "\n",
    "# Show a sample of mismatched cities\n",
    "print(conflicts[['Location Code', 'City_x', 'City_y', 'zip']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce46918-a1a9-4b2f-bc6d-a7154ad073a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of your city conflicts aren’t really “true” conflicts but spelling/style/abbreviation differences or neighborhood vs. city names.\n",
    "\n",
    "# We'll handle these smartly using the following stepwise approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c76f7f-d080-40b4-9b8b-afee95af714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Use ZIP Codes to Auto-Resolve Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64606523-aa3d-4f81-ac23-37721b326a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining conflicts after ZIP-based resolution: 688\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"state_resolution_audit.csv\")\n",
    "\n",
    "# If city_y is missing, first fill it with city_x\n",
    "df[\"City_y\"] = df[\"City_y\"].fillna(df[\"City_x\"])\n",
    "\n",
    "# Create a ZIP → most common city mapping\n",
    "zip_city_map = (\n",
    "    df.groupby(\"zip\")[\"City_y\"]\n",
    "    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Apply this mapping to create a unified city column\n",
    "df[\"Unified_City\"] = df[\"zip\"].map(zip_city_map)\n",
    "\n",
    "# Check how many are still conflicting\n",
    "df[\"city_conflict\"] = (df[\"City_x\"].str.upper() != df[\"Unified_City\"].str.upper())\n",
    "print(\"Remaining conflicts after ZIP-based resolution:\", df[\"city_conflict\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae0b56-8983-4fb2-87b1-f3b93939ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — Fuzzy Matching for Close Spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37573756-b5a7-4a97-b6fd-355d0c8d67af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def pick_best_city(row):\n",
    "    city_x = str(row[\"City_x\"]).upper()\n",
    "    unified = str(row[\"Unified_City\"]).upper()\n",
    "    score = fuzz.ratio(city_x, unified)\n",
    "    return unified if score >= 85 else city_x\n",
    "\n",
    "df[\"Final_City\"] = df.apply(pick_best_city, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22325914-c991-4553-a13f-283292497adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Save Final Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527623e-993c-4e14-8d9c-4f16bb8db98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ City conflicts resolved and saved to city_resolution_audit.csv\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=[\"city_conflict\"], inplace=True, errors=\"ignore\")\n",
    "df.to_csv(\"city_resolution_audit.csv\", index=False)\n",
    "print(\"✅ City conflicts resolved and saved to city_resolution_audit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0067a4e-a0df-46a1-98f2-05b4b7502e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 — Recheck Conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf1a6f-4a20-424b-8344-4e7c28d8d27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final remaining conflicts: 38\n",
      "     Location Code              City_x                City_y  \\\n",
      "348         NC2615       WINSTON SALEM         Winston-Salem   \n",
      "384         NC0113       WINSTON SALEM         Winston-Salem   \n",
      "462         OH2481        BEAVER CREEK           Beavercreek   \n",
      "1017        AR0066  HELENA-WEST HELENA  Helena - West Helena   \n",
      "1716        HI8562              LIHU'E                 Lihue   \n",
      "1886        MI3087    ST. CLAIR SHORES    Saint Clair Shores   \n",
      "2066        HI7553              LIHU'E                 Lihue   \n",
      "2160        MI3033     SAULT STE MARIE    Sault Sainte Marie   \n",
      "2216        SC1374         MT PLEASANT        Mount Pleasant   \n",
      "2244        NC1383       WINSTON SALEM         Winston-Salem   \n",
      "2304        HI7793              KEA'AU                 Keaau   \n",
      "2535        PA0627        WILKES BARRE          Wilkes-Barre   \n",
      "2588        PA0921        WILKES BARRE          Wilkes-Barre   \n",
      "2716        OH1816     ST. CLAIRSVILLE     Saint Clairsville   \n",
      "2995        PA0920        WILKES BARRE          Wilkes-Barre   \n",
      "3107        MN1711       SOUTH ST PAUL      South Saint Paul   \n",
      "3229        PA0477        WILKES BARRE          Wilkes-Barre   \n",
      "3543        PA0808             DU BOIS                DuBois   \n",
      "3596        PA0591        WILKES BARRE          Wilkes-Barre   \n",
      "4264        HI8328              LIHU'E                 Lihue   \n",
      "4308        MI2184     SAULT STE MARIE    Sault Sainte Marie   \n",
      "4544        MD0259        DISTRICT HTS      District Heights   \n",
      "4592        MN1726       SOUTH ST PAUL      South Saint Paul   \n",
      "4784        IL2628            O'FALLON              O Fallon   \n",
      "5167        VT8116       ST. JOHNSBURY          St Johnsbury   \n",
      "5254        HI5889              LIHU'E                 Lihue   \n",
      "6265        PA0676        WILKES BARRE          Wilkes-Barre   \n",
      "6791        MI0724     SAULT STE MARIE    Sault Sainte Marie   \n",
      "6939        NC2711       WINSTON SALEM         Winston-Salem   \n",
      "6991        NC2595       WINSTON SALEM         Winston-Salem   \n",
      "7040        MI2151     SAULT STE MARIE    Sault Sainte Marie   \n",
      "7762        SC2318         MT PLEASANT        Mount Pleasant   \n",
      "7853        PA0630        WILKES BARRE          Wilkes-Barre   \n",
      "7979        MI3019     SAULT STE MARIE    Sault Sainte Marie   \n",
      "8044        VT8118       ST. JOHNSBURY          St Johnsbury   \n",
      "8093        HI7501              LIHU'E                 Lihue   \n",
      "8194        MI2145     SAULT STE MARIE    Sault Sainte Marie   \n",
      "8249        SC2172     HILTON HEAD ISL    Hilton Head Island   \n",
      "\n",
      "                Final_City    zip  \n",
      "348          WINSTON-SALEM  27101  \n",
      "384          WINSTON-SALEM  27101  \n",
      "462            BEAVERCREEK  45431  \n",
      "1017  HELENA - WEST HELENA  72342  \n",
      "1716                 LIHUE  96766  \n",
      "1886    SAINT CLAIR SHORES  48080  \n",
      "2066                 LIHUE  96766  \n",
      "2160    SAULT SAINTE MARIE  49783  \n",
      "2216        MOUNT PLEASANT  29464  \n",
      "2244         WINSTON-SALEM  27101  \n",
      "2304                 KEAAU  96749  \n",
      "2535          WILKES-BARRE  18702  \n",
      "2588          WILKES-BARRE  18702  \n",
      "2716     SAINT CLAIRSVILLE  43950  \n",
      "2995          WILKES-BARRE  18702  \n",
      "3107      SOUTH SAINT PAUL  55075  \n",
      "3229          WILKES-BARRE  18702  \n",
      "3543                DUBOIS  15801  \n",
      "3596          WILKES-BARRE  18702  \n",
      "4264                 LIHUE  96766  \n",
      "4308    SAULT SAINTE MARIE  49783  \n",
      "4544      DISTRICT HEIGHTS  20747  \n",
      "4592      SOUTH SAINT PAUL  55075  \n",
      "4784              O FALLON  62269  \n",
      "5167          ST JOHNSBURY   5819  \n",
      "5254                 LIHUE  96766  \n",
      "6265          WILKES-BARRE  18701  \n",
      "6791    SAULT SAINTE MARIE  49783  \n",
      "6939         WINSTON-SALEM  27101  \n",
      "6991         WINSTON-SALEM  27103  \n",
      "7040    SAULT SAINTE MARIE  49783  \n",
      "7762        MOUNT PLEASANT  29464  \n",
      "7853          WILKES-BARRE  18702  \n",
      "7979    SAULT SAINTE MARIE  49783  \n",
      "8044          ST JOHNSBURY   5819  \n",
      "8093                 LIHUE  96766  \n",
      "8194    SAULT SAINTE MARIE  49783  \n",
      "8249    HILTON HEAD ISLAND  29926  \n"
     ]
    }
   ],
   "source": [
    "conflicts = df[df[\"City_x\"].str.upper() != df[\"Final_City\"].str.upper()]\n",
    "print(\"Final remaining conflicts:\", len(conflicts))\n",
    "print(conflicts[[\"Location Code\", \"City_x\", \"City_y\", \"Final_City\", \"zip\"]].head(38))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aff9ae-26dd-4bd8-9847-66c5d37ca102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolveddddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaae1cb-84e5-4f15-b138-7bcc05fcb533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2dfa01-0506-4f6b-aaf7-172c4e411b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df647357-a1df-4a39-be7d-bc08edd4c392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d23bb-d826-48cb-aa29-952211e9da5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447e6fd-e3ca-4bc6-8c39-28e40902cce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaeb32f-9492-4cdd-886a-c8b9e09076ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41112a-c611-4cb9-a87e-47b473c89c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99843c44-90a4-48c1-a6d6-1749b3394f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042253e-5b15-4c82-9c9d-772b7bb04963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Load the cleaned merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6d5a9-b082-4171-ae9b-eb71793492a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing price_latest values: 616\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset after state conflict resolution\n",
    "df = pd.read_csv(\"conflict_resolved.csv\")\n",
    "\n",
    "# Check missing price_latest values\n",
    "missing_price = df['price_latest'].isna().sum()\n",
    "print(f\"Missing price_latest values: {missing_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b4757-8b78-4e17-b9a0-4999db6cd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — Check how missing price data is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb27c7-614d-493b-a6c3-88d0d98dfdb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Unified_State'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Count missing values by state\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m missing_by_state \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice_latest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()]\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnified_State\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing price counts by state:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, missing_by_state)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Count missing values by GSA region\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   9184\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9185\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   9186\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   9187\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   9188\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[0;32m   9189\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   9190\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[0;32m   9191\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   9192\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   9193\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[0;32m   1330\u001b[0m         obj,\n\u001b[0;32m   1331\u001b[0m         keys,\n\u001b[0;32m   1332\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   1333\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   1334\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   1335\u001b[0m         observed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m observed,\n\u001b[0;32m   1336\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[0;32m   1337\u001b[0m     )\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Unified_State'"
     ]
    }
   ],
   "source": [
    "# Count missing values by state\n",
    "missing_by_state = df[df['price_latest'].isna()].groupby('Unified_State').size().sort_values(ascending=False)\n",
    "print(\"Missing price counts by state:\\n\", missing_by_state)\n",
    "\n",
    "# Count missing values by GSA region\n",
    "missing_by_region = df[df['price_latest'].isna()].groupby('GSA Region').size().sort_values(ascending=False)\n",
    "print(\"\\nMissing price counts by GSA region:\\n\", missing_by_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd7152-04c9-483d-bc23-aae366f43dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Fill missing prices intelligently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ec10a-1ce9-4b72-8662-50b3c284a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of median prices by zip\n",
    "zip_price_map = df.groupby(\"zip\")[\"price_latest\"].median()\n",
    "\n",
    "# Fill missing prices using the same zip median price\n",
    "df[\"price_latest\"] = df.apply(\n",
    "    lambda x: zip_price_map[x[\"zip\"]] if pd.isna(x[\"price_latest\"]) and x[\"zip\"] in zip_price_map else x[\"price_latest\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6399ba-e79c-4b0f-aa74-23517a354b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
